{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bead3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notas totales: 50 | GT: 50 | Intersección: 50 | Usadas: 20\n",
      "✔ Muestra guardada: C:\\Users\\hered\\Desktop\\TFM\\TFM\\TFM2\\outputs\\notes_sample_20.csv\n",
      "✔ Modelo válido en HF: microsoft/BioGPT  (sha: eb0d815)\n",
      "\n",
      " Ejecutando experimento para: microsoft/BioGPT\n",
      "\n",
      "Cargando tokenizer: microsoft/BioGPT\n",
      "Cargando modelo OpenVINO en GPU (exportando si es necesario)...\n",
      " Modelo operativo en GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencia v1_simple: 100%|██████████| 20/20 [09:52<00:00, 29.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Guardado: C:\\Users\\hered\\Desktop\\TFM\\TFM\\TFM2\\outputs/pred_v1_simple_BioGPT_n20.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencia v2_estricto: 100%|██████████| 20/20 [09:50<00:00, 29.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Guardado: C:\\Users\\hered\\Desktop\\TFM\\TFM\\TFM2\\outputs/pred_v2_estricto_BioGPT_n20.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencia v3_fewshot: 100%|██████████| 20/20 [11:03<00:00, 33.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Guardado: C:\\Users\\hered\\Desktop\\TFM\\TFM\\TFM2\\outputs/pred_v3_fewshot_BioGPT_n20.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencia v4_encadenado: 100%|██████████| 20/20 [16:13<00:00, 48.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Users\\hered\\Desktop\\TFM\\TFM\\TFM2\\outputs/pred_v4_encadenado_BioGPT_n20.csv\n",
      "\n",
      " Ficheros de predicción generados para microsoft/BioGPT\n",
      " - C:\\Users\\hered\\Desktop\\TFM\\TFM\\TFM2\\outputs/pred_v1_simple_BioGPT_n20.csv\n",
      " - C:\\Users\\hered\\Desktop\\TFM\\TFM\\TFM2\\outputs/pred_v2_estricto_BioGPT_n20.csv\n",
      " - C:\\Users\\hered\\Desktop\\TFM\\TFM\\TFM2\\outputs/pred_v3_fewshot_BioGPT_n20.csv\n",
      " - C:\\Users\\hered\\Desktop\\TFM\\TFM\\TFM2\\outputs/pred_v4_encadenado_BioGPT_n20.csv\n",
      " Modelo válido en HF: microsoft/BioGPT-Large  (sha: c6a5136)\n",
      "\n",
      " Ejecutando experimento para: microsoft/BioGPT-Large\n",
      "\n",
      "Cargando tokenizer: microsoft/BioGPT-Large\n",
      "Cargando modelo OpenVINO en GPU (exportando si es necesario)...\n",
      "INFO:nncf:Statistics of the bitwidth distribution:\n",
      "+---------------------------+-----------------------------+----------------------------------------+\n",
      "| Weight compression mode   | % all parameters (layers)   | % ratio-defining parameters (layers)   |\n",
      "+===========================+=============================+========================================+\n",
      "| int8_asym                 | 100% (290 / 290)            | 100% (290 / 290)                       |\n",
      "+---------------------------+-----------------------------+----------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Modelo operativo en GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencia v1_simple:  35%|███▌      | 7/20 [23:22<43:24, 200.38s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 503\u001b[39m\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, r \u001b[38;5;129;01min\u001b[39;00m tqdm(notes_run.iterrows(), total=\u001b[38;5;28mlen\u001b[39m(notes_run), desc=\u001b[33m\"\u001b[39m\u001b[33mInferencia v1_simple\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    502\u001b[39m     pid, note = r[\u001b[33m\"\u001b[39m\u001b[33mpatient_id\u001b[39m\u001b[33m\"\u001b[39m], \u001b[38;5;28mstr\u001b[39m(r[\u001b[33m\"\u001b[39m\u001b[33mpatient\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m     H,W,B_from,B = \u001b[43mrun_triplet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSYSTEM_SIMPLE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_generate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    504\u001b[39m     rows.append({\n\u001b[32m    505\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpatient_id\u001b[39m\u001b[33m\"\u001b[39m: pid,\n\u001b[32m    506\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mheight_m_pred\u001b[39m\u001b[33m\"\u001b[39m: H \u001b[38;5;28;01mif\u001b[39;00m H \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m np.nan,\n\u001b[32m   (...)\u001b[39m\u001b[32m    512\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmodel_used\u001b[39m\u001b[33m\"\u001b[39m: ALT_MODEL_ID\n\u001b[32m    513\u001b[39m     })\n\u001b[32m    514\u001b[39m save_csv(rows, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/pred_v1_simple_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mALT_MODEL_ID.split(\u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m)[-\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_n\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(notes_run)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 321\u001b[39m, in \u001b[36mrun_triplet\u001b[39m\u001b[34m(note_text, system_text, llm_generate, apply_chat_template)\u001b[39m\n\u001b[32m    319\u001b[39m prompt = gen_prompt(apply_chat_template, system_text, chunk)\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(ATTEMPTS_PER_WIN):\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m     obj = safe_json(\u001b[43mllm_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    322\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    323\u001b[39m     h,w,b = obj.get(\u001b[33m\"\u001b[39m\u001b[33mheight_m\u001b[39m\u001b[33m\"\u001b[39m), obj.get(\u001b[33m\"\u001b[39m\u001b[33mweight_kg\u001b[39m\u001b[33m\"\u001b[39m), obj.get(\u001b[33m\"\u001b[39m\u001b[33mbmi\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 174\u001b[39m, in \u001b[36mget_ov_model_and_tokenizer.<locals>.llm_generate\u001b[39m\u001b[34m(prompt, max_new, temperature, top_p, do_sample)\u001b[39m\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mllm_generate\u001b[39m(prompt: \u001b[38;5;28mstr\u001b[39m, max_new=\u001b[32m160\u001b[39m, temperature=\u001b[32m0.8\u001b[39m, top_p=\u001b[32m0.95\u001b[39m, do_sample=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    172\u001b[39m     \u001b[38;5;66;03m# Un poco más \"creativo\" para evitar respuestas vacías en modelos no-instruct\u001b[39;00m\n\u001b[32m    173\u001b[39m     inputs = tok(prompt, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     out_ids = \u001b[43mov_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_new\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmin_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.05\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtok\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mov_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mov_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtok\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tok.decode(out_ids[\u001b[32m0\u001b[39m][inputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m].shape[\u001b[32m1\u001b[39m]:], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m).strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hered\\Desktop\\TFM\\TFM\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hered\\Desktop\\TFM\\TFM\\.venv\\Lib\\site-packages\\optimum\\intel\\openvino\\modeling_decoder.py:756\u001b[39m, in \u001b[36mOVModelForCausalLM.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[39m\n\u001b[32m    754\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_beam_search:\n\u001b[32m    755\u001b[39m     \u001b[38;5;28mself\u001b[39m._first_iter_beam_search = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m756\u001b[39m result = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    757\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    758\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    759\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    760\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    761\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    762\u001b[39m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    763\u001b[39m \u001b[43m    \u001b[49m\u001b[43massistant_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    764\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    765\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    766\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    767\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    768\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    769\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hered\\Desktop\\TFM\\TFM\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hered\\Desktop\\TFM\\TFM\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2215\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[39m\n\u001b[32m   2207\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2208\u001b[39m         input_ids=input_ids,\n\u001b[32m   2209\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2210\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2211\u001b[39m         **model_kwargs,\n\u001b[32m   2212\u001b[39m     )\n\u001b[32m   2214\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2215\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2216\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2217\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2218\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2219\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2220\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2221\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2222\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2225\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2226\u001b[39m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[32m   2227\u001b[39m     beam_scorer = BeamSearchScorer(\n\u001b[32m   2228\u001b[39m         batch_size=batch_size,\n\u001b[32m   2229\u001b[39m         num_beams=generation_config.num_beams,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2234\u001b[39m         max_length=generation_config.max_length,\n\u001b[32m   2235\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hered\\Desktop\\TFM\\TFM\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:3206\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3203\u001b[39m model_inputs.update({\u001b[33m\"\u001b[39m\u001b[33moutput_hidden_states\u001b[39m\u001b[33m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[32m   3205\u001b[39m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3206\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3208\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3209\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3210\u001b[39m     outputs,\n\u001b[32m   3211\u001b[39m     model_kwargs,\n\u001b[32m   3212\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3213\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hered\\Desktop\\TFM\\TFM\\.venv\\Lib\\site-packages\\optimum\\modeling_base.py:109\u001b[39m, in \u001b[36mOptimizedModel.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hered\\Desktop\\TFM\\TFM\\.venv\\Lib\\site-packages\\optimum\\intel\\openvino\\modeling_decoder.py:576\u001b[39m, in \u001b[36mOVModelForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, past_key_values, position_ids, token_type_ids, **kwargs)\u001b[39m\n\u001b[32m    574\u001b[39m \u001b[38;5;66;03m# Run inference\u001b[39;00m\n\u001b[32m    575\u001b[39m \u001b[38;5;28mself\u001b[39m.request.start_async(inputs, share_inputs=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    577\u001b[39m logits = torch.from_numpy(\u001b[38;5;28mself\u001b[39m.request.get_tensor(\u001b[33m\"\u001b[39m\u001b[33mlogits\u001b[39m\u001b[33m\"\u001b[39m).data).clone().to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stateful:\n\u001b[32m    579\u001b[39m     \u001b[38;5;66;03m# Need a marker to differentiate the first generate iteration from the others in\u001b[39;00m\n\u001b[32m    580\u001b[39m     \u001b[38;5;66;03m# the first condition at the function beginning above.\u001b[39;00m\n\u001b[32m    581\u001b[39m     \u001b[38;5;66;03m# It should be something that is not None and it should be True when converted to Boolean.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# inferencia_local_openvino.py\n",
    "#  INFERENCIA (4 VARIANTES DE PROMPT) — MULTI-MODELO CLÍNICO (LOCAL, OpenVINO)\n",
    "# Modelos ejemplo:\n",
    "#   - microsoft/BioGPT\n",
    "#   - microsoft/BioGPT-Large\n",
    "# Backend: OpenVINO (Intel GPU si está, fallback CPU). Guarda CSVs localmente.\n",
    "\n",
    "import os, re, json, random, warnings, sys\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#  Dependencias de runtime \n",
    "# Recomendado (Windows, Py3.12):\n",
    "#   python -m pip install \"numpy<2.1\" \"transformers==4.46.2\" \"optimum-intel[openvino]==1.26.0\" \"openvino>=2025.1.0\" \"accelerate>=0.34\" \"huggingface_hub>=0.24\"\n",
    "from transformers import AutoTokenizer\n",
    "from optimum.intel.openvino import OVModelForCausalLM\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "try:\n",
    "    from huggingface_hub.utils import HfHubHTTPError\n",
    "except Exception:\n",
    "    try:\n",
    "        from huggingface_hub.utils._errors import HfHubHTTPError\n",
    "    except Exception:\n",
    "        HfHubHTTPError = Exception\n",
    "\n",
    "#  Config CLI (compatible con Jupyter/VSCode) \n",
    "def parse_args(argv=None):\n",
    "    ap = argparse.ArgumentParser(add_help=True)\n",
    "    ap.add_argument(\"--notes\", default=r\"C:\\Users\\hered\\Desktop\\TFM\\TFM\\TFM2\\sample_notes_imc.csv\", help=\"CSV con las 20 notas\")\n",
    "    ap.add_argument(\"--gt\",    default=r\"C:\\Users\\hered\\Desktop\\TFM\\TFM\\TFM2\\gt_imc_final.csv\",     help=\"CSV con GT\")\n",
    "    ap.add_argument(\"--outdir\",default=r\"C:\\Users\\hered\\Desktop\\TFM\\TFM\\TFM2\\outputs\",              help=\"Directorio de salida\")\n",
    "    ap.add_argument(\"--n\", type=int, default=20, help=\"Número de notas a usar\")\n",
    "    ap.add_argument(\"--device_pref\", default=\"GPU\", choices=[\"GPU\",\"CPU\"], help=\"Preferencia de dispositivo OpenVINO\")\n",
    "    ap.add_argument(\"--models\", nargs=\"*\", default=[\n",
    "        \"microsoft/BioGPT\",\n",
    "        \"microsoft/BioGPT-Large\",\n",
    "    ], help=\"Lista de modelos HF (IDs válidos)\")\n",
    "    ap.add_argument(\"--hf_token\", default=os.getenv(\"HUGGINGFACE_HUB_TOKEN\", None), help=\"Token HF (si el repo es privado/gated).\")\n",
    "    ap.add_argument(\"--seed\", type=int, default=7)\n",
    "    ap.add_argument(\"--limit_windows\", type=int, default=6)\n",
    "    ap.add_argument(\"--attempts_per_window\", type=int, default=3)\n",
    "\n",
    "    if argv is None:\n",
    "        argv = [] if \"ipykernel\" in sys.modules else None\n",
    "\n",
    "    args, _unknown = ap.parse_known_args(argv)\n",
    "    return args\n",
    "\n",
    "args = parse_args()\n",
    "random.seed(args.seed); np.random.seed(args.seed)\n",
    "\n",
    "# RUTAS FIJAS (como pediste)\n",
    "PATH_NOTES = r\"C:\\Users\\hered\\Desktop\\TFM\\TFM\\TFM2\\sample_notes_imc.csv\"\n",
    "PATH_GT    = r\"C:\\Users\\hered\\Desktop\\TFM\\TFM\\TFM2\\gt_imc_final.csv\"\n",
    "OUT_DIR    = r\"C:\\Users\\hered\\Desktop\\TFM\\TFM\\TFM2\\outputs\"\n",
    "\n",
    "N_NOTES    = args.n\n",
    "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "SAMPLE_OUT = str(Path(OUT_DIR) / f\"notes_sample_{N_NOTES}.csv\")\n",
    "\n",
    "#  Helpers HF \n",
    "def validate_model_or_fail(repo_id: str, token: str | None = None):\n",
    "    api = HfApi()\n",
    "    try:\n",
    "        info = api.model_info(repo_id, token=token)\n",
    "        if not info:\n",
    "            raise SystemExit(f\" No se pudo obtener info del repo '{repo_id}'.\")\n",
    "        print(f\"✔ Modelo válido en HF: {repo_id}  (sha: {getattr(info, 'sha', 'n/a')[:7]})\")\n",
    "    except HfHubHTTPError as e:\n",
    "        code = getattr(e.response, \"status_code\", None)\n",
    "        if code == 404:\n",
    "            raise SystemExit(f\" Repo no encontrado en HF: {repo_id}\\nCorrige el ID (p.ej. 'microsoft/BioGPT').\")\n",
    "        elif code == 401:\n",
    "            raise SystemExit(\n",
    "                f\" 401 Unauthorized para {repo_id}.\\n\"\n",
    "                f\"- Ejecuta `hf auth login` o pasa --hf_token.\\n\"\n",
    "                f\"- Si es un repo ‘gated’, acepta términos en su página.\"\n",
    "            )\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "#  Cargar datos \n",
    "assert Path(PATH_NOTES).exists(), f\"Falta {PATH_NOTES}\"\n",
    "assert Path(PATH_GT).exists(), f\"Falta {PATH_GT}\"\n",
    "\n",
    "def load_notes(csv_path):\n",
    "    df = pd.read_csv(csv_path, dtype={\"patient_id\": str})\n",
    "    cols = {c.lower().strip(): c for c in df.columns}\n",
    "    pid = next((cols[c] for c in [\"patient_id\",\"id\",\"pid\",\"subject_id\"] if c in cols), None)\n",
    "    txt = next((cols[c] for c in [\"patient\",\"note_text\",\"note\",\"text\"] if c in cols), None)\n",
    "    assert pid and txt, \"Necesito columnas ['patient_id', 'patient'/'note_text']\"\n",
    "    df = df.rename(columns={pid:\"patient_id\", txt:\"patient\"})[[\"patient_id\",\"patient\"]]\n",
    "    df[\"patient_id\"] = df[\"patient_id\"].astype(str).str.strip()\n",
    "    df[\"patient\"]    = df[\"patient\"].astype(str)\n",
    "    return df\n",
    "\n",
    "notes_full = load_notes(PATH_NOTES)\n",
    "gt = pd.read_csv(PATH_GT, dtype={\"patient_id\": str})\n",
    "gt[\"patient_id\"] = gt[\"patient_id\"].astype(str).str.strip()\n",
    "for c in [\"height_m_true\",\"weight_kg_true\",\"BMI_true\"]:\n",
    "    if c in gt.columns:\n",
    "        gt[c] = pd.to_numeric(gt[c], errors=\"coerce\")\n",
    "gt[\"bmi_explicit_in_note\"] = gt[\"bmi_explicit_in_note\"].astype(bool) if \"bmi_explicit_in_note\" in gt.columns else False\n",
    "\n",
    "inter = notes_full.merge(gt[[\"patient_id\"]], on=\"patient_id\", how=\"inner\")\n",
    "assert len(inter) >= N_NOTES, f\"No hay suficientes IDs comunes para N={N_NOTES}.\"\n",
    "TXT_COL = \"patient\" if \"patient\" in inter.columns else (\"note_text\" if \"note_text\" in inter.columns else None)\n",
    "assert TXT_COL is not None, f\"Falta columna de texto en {PATH_NOTES}: {inter.columns.tolist()}\"\n",
    "notes_20 = (\n",
    "    inter[[\"patient_id\", TXT_COL]]\n",
    "    .rename(columns={TXT_COL: \"patient\"})\n",
    "    .drop_duplicates(\"patient_id\")\n",
    "    .head(N_NOTES)\n",
    "    .copy()\n",
    ")\n",
    "notes_20.to_csv(SAMPLE_OUT, index=False)\n",
    "print(f\"Notas totales: {len(notes_full)} | GT: {len(gt)} | Intersección: {len(inter)} | Usadas: {len(notes_20)}\")\n",
    "print(f\"✔ Muestra guardada: {SAMPLE_OUT}\")\n",
    "\n",
    "#  Modelo: OpenVINO (GPU Intel si hay; si falla → CPU) \n",
    "def get_ov_model_and_tokenizer(model_id: str, device_pref: str = \"GPU\", token: str | None = None):\n",
    "    \"\"\"\n",
    "    Carga tokenizer y modelo OpenVINO. Intenta GPU Intel; si falla, cae a CPU.\n",
    "    Devuelve: (tokenizer, llm_generate, apply_chat_template)\n",
    "    \"\"\"\n",
    "    print(f\"\\nCargando tokenizer: {model_id}\")\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True, token=token)\n",
    "    if tok.pad_token_id is None and tok.eos_token_id is not None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "\n",
    "    device = \"GPU\" if device_pref.upper() == \"GPU\" else \"CPU\"\n",
    "    print(f\"Cargando modelo OpenVINO en {device} (exportando si es necesario)...\")\n",
    "    ov_model = OVModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        export=True,\n",
    "        device=device,\n",
    "        compile=True,\n",
    "        trust_remote_code=True,\n",
    "        ov_config={\"CACHE_DIR\": str(Path(OUT_DIR) / \"ov_cache\")},\n",
    "        token=token,\n",
    "    )\n",
    "    if getattr(ov_model.config, \"pad_token_id\", None) is None and tok.eos_token_id is not None:\n",
    "        ov_model.config.pad_token_id = tok.eos_token_id\n",
    "\n",
    "    # Probar inferencia mínima; si falla en GPU, reintentar en CPU.\n",
    "    try:\n",
    "        test_ids = tok(\"ok\", return_tensors=\"pt\").input_ids\n",
    "        _ = ov_model.generate(test_ids, max_new_tokens=1)\n",
    "        print(f\" Modelo operativo en {device}\")\n",
    "    except Exception as e:\n",
    "        if device == \"GPU\":\n",
    "            print(f\" Falló en GPU ({e}). Reintentando en CPU…\")\n",
    "            ov_model = OVModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                export=True,\n",
    "                device=\"CPU\",\n",
    "                compile=True,\n",
    "                trust_remote_code=True,\n",
    "                ov_config={\"CACHE_DIR\": str(Path(OUT_DIR) / \"ov_cache\")},\n",
    "                token=token,\n",
    "            )\n",
    "            print(\" Modelo operativo en CPU\")\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    def llm_generate(prompt: str, max_new=160, temperature=0.8, top_p=0.95, do_sample=True):\n",
    "        # Un poco más \"creativo\" para evitar respuestas vacías en modelos no-instruct\n",
    "        inputs = tok(prompt, return_tensors=\"pt\")\n",
    "        out_ids = ov_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new,\n",
    "            min_new_tokens=8,\n",
    "            do_sample=do_sample,\n",
    "            temperature=float(temperature),\n",
    "            top_p=float(top_p),\n",
    "            repetition_penalty=1.05,\n",
    "            eos_token_id=(tok.eos_token_id or ov_model.config.eos_token_id),\n",
    "            pad_token_id=(ov_model.config.pad_token_id or tok.eos_token_id),\n",
    "        )\n",
    "        return tok.decode(out_ids[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True).strip()\n",
    "\n",
    "    def apply_chat_template(system_text: str, user_text: str):\n",
    "        \"\"\"\n",
    "        Si el tokenizer trae chat_template, úsalo. Si no (p. ej., BioGPT),\n",
    "        construye un prompt Q/A simple.\n",
    "        \"\"\"\n",
    "        has_apply = hasattr(tok, \"apply_chat_template\")\n",
    "        has_template = bool(getattr(tok, \"chat_template\", None))\n",
    "        if has_apply and has_template:\n",
    "            return tok.apply_chat_template(\n",
    "                [\n",
    "                    {\"role\": \"system\", \"content\": system_text},\n",
    "                    {\"role\": \"user\",  \"content\": user_text},\n",
    "                ],\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "            )\n",
    "        # Fallback Q/A\n",
    "        return (\n",
    "            \"Task:\\n\" + system_text.strip() + \"\\n\\n\" +\n",
    "            \"Input:\\n\" + user_text.strip() + \"\\n\" +\n",
    "            \"Output:\\n\"\n",
    "        )\n",
    "\n",
    "    return tok, llm_generate, apply_chat_template\n",
    "\n",
    "#  Helpers de ventana / plausibilidad n",
    "WIN, STRIDE = 1100, 800\n",
    "UNIT_TOKENS = [\" cm\",\" m\",\"meter\",\"metre\",\"ft\",\" in\",\"inch\",\"kg\",\" lb\",\"lbs\",\"pound\",\" stone\",\" st\",\"bmi\",\"BMI\",\"weight\",\"height\"]\n",
    "\n",
    "def window_iter(text, win=WIN, stride=STRIDE):\n",
    "    t = str(text); n = len(t)\n",
    "    if n <= win:\n",
    "        yield 0, t; return\n",
    "    for i in range(0, n, stride):\n",
    "        yield i, t[i:i+win]\n",
    "        if i+win >= n: break\n",
    "\n",
    "def has_unit_token(s: str):\n",
    "    sl = (s or \"\").lower()\n",
    "    return any(tok.strip().lower() in sl for tok in UNIT_TOKENS)\n",
    "\n",
    "H_MIN, H_MAX = 1.2, 2.2\n",
    "W_MIN, W_MAX = 30, 300\n",
    "BMI_MIN, BMI_MAX = 10, 80\n",
    "BMI_TOL = 0.5\n",
    "\n",
    "def is_num(x):\n",
    "    try:\n",
    "        return np.isfinite(float(x))\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def clip_plausible(h, w, b):\n",
    "    try:\n",
    "        if is_num(h) and not (H_MIN <= float(h) <= H_MAX): h = None\n",
    "    except: h = None\n",
    "    try:\n",
    "        if is_num(w) and not (W_MIN <= float(w) <= W_MAX): w = None\n",
    "    except: w = None\n",
    "    try:\n",
    "        if is_num(b) and not (BMI_MIN <= float(b) <= BMI_MAX): b = None\n",
    "    except: b = None\n",
    "    return h, w, b\n",
    "\n",
    "def recompute_bmi(h, w):\n",
    "    try:\n",
    "        h = float(h); w = float(w)\n",
    "        if h > 0: return round(w/(h*h), 2)\n",
    "    except: pass\n",
    "    return None\n",
    "\n",
    "#  4 VARIANTES DE PROMPT n",
    "# 1) SIMPLE (tripleta)\n",
    "SYSTEM_SIMPLE = (\n",
    "    \"You are a careful clinical extractor. From the GIVEN WINDOW ONLY, return STRICT JSON with normalized SI values:\\n\"\n",
    "    \"{ \\\"height_m\\\": <float|null>, \\\"weight_kg\\\": <float|null>, \\\"bmi\\\": <float|null> }\\n\"\n",
    "    \"Rules: Use ONLY numbers present; convert units to SI; if either height or weight is missing, bmi=null. Output JSON only.\"\n",
    ")\n",
    "\n",
    "# 2) ESTRICTO (unidades explícitas + verificación)\n",
    "SYSTEM_STRICT = (\n",
    "    \"You are a clinical extractor and verifier. From the GIVEN WINDOW ONLY, return STRICT JSON:\\n\"\n",
    "    \"{ \\\"height_m\\\": <float|null>, \\\"weight_kg\\\": <float|null>, \\\"bmi\\\": <float|null> }\\n\"\n",
    "    \"Rules:\\n\"\n",
    "    \"• Use ONLY numbers with explicit units (cm/m/ft-in → m; kg/lb/stone → kg).\\n\"\n",
    "    \"• Plausibility: 1.20 ≤ height_m ≤ 2.20, 30 ≤ weight_kg ≤ 300, 10 ≤ bmi ≤ 80.\\n\"\n",
    "    \"• If both H & W exist, compute bmi=kg/(m^2) (2 decimals) and prefer this over any conflicting BMI text.\\n\"\n",
    "    \"• If inconsistent, set bmi=null. Output JSON only.\"\n",
    ")\n",
    "\n",
    "# 3) FEW-SHOT (ejemplos)\n",
    "SYSTEM_FEWSHOT = SYSTEM_SIMPLE  # usamos mismas reglas, pero con ejemplos en el prompt builder\n",
    "FEW_SHOTS = [\n",
    "    (\"A 60-year-old woman, height 165 cm and weight 68 kg.\",\n",
    "     \"{\\\"height_m\\\": 1.65, \\\"weight_kg\\\": 68.0, \\\"bmi\\\": 24.98}\"),\n",
    "    (\"Male, 1.80 m, 90 kg; BMI not explicitly stated in text.\",\n",
    "     \"{\\\"height_m\\\": 1.80, \\\"weight_kg\\\": 90.0, \\\"bmi\\\": 27.78}\"),\n",
    "    (\"Patient reports good energy. No numeric measurements present.\",\n",
    "     \"{\\\"height_m\\\": null, \\\"weight_kg\\\": null, \\\"bmi\\\": null}\"),\n",
    "]\n",
    "\n",
    "# 4) ENCADENADO (SPAN -> NORM -> JUEZ)\n",
    "# (definido dentro de funciones run_chain_on_window / run_chain_on_note)\n",
    "\n",
    "#  Utilidades de generación / parsing \n",
    "def safe_json(text: str):\n",
    "    if not text: return None\n",
    "    s = text.strip()\n",
    "    if s.startswith(\"```\"):\n",
    "        try:\n",
    "            s = s.split(\"```\", 1)[-1]\n",
    "            if \"```\" in s: s = s.split(\"```\",1)[0]\n",
    "        except: pass\n",
    "    a, b = s.find(\"{\"), s.rfind(\"}\")\n",
    "    if a!=-1 and b!=-1 and b>a: s = s[a:b+1]\n",
    "    s = s.replace(\"None\",\"null\").replace(\"NaN\",\"null\").replace(\",}\", \"}\")\n",
    "    try:\n",
    "        obj = json.loads(s)\n",
    "        return obj if isinstance(obj, dict) else None\n",
    "    except: return None\n",
    "\n",
    "def gen_prompt(apply_chat_template, system_text, window_text):\n",
    "    return apply_chat_template(system_text, \"NOTE WINDOW:\\n\"+window_text+\"\\n\\nJSON ONLY\")\n",
    "\n",
    "#  Ejecutores comunes \n",
    "ATTEMPTS_PER_WIN = args.attempts_per_window\n",
    "N_WINDOWS_MAX = args.limit_windows\n",
    "\n",
    "def run_triplet(note_text: str, system_text: str, llm_generate, apply_chat_template):\n",
    "    wins = sorted([(s,c) for s,c in window_iter(note_text)], key=lambda x: int(not has_unit_token(x[1])))\n",
    "    best={\"h\":None,\"w\":None,\"b\":None,\"score\":-1e9}\n",
    "    for _, chunk in wins[:N_WINDOWS_MAX]:\n",
    "        prompt = gen_prompt(apply_chat_template, system_text, chunk)\n",
    "        for _ in range(ATTEMPTS_PER_WIN):\n",
    "            obj = safe_json(llm_generate(prompt))\n",
    "            if obj is None: continue\n",
    "            h,w,b = obj.get(\"height_m\"), obj.get(\"weight_kg\"), obj.get(\"bmi\")\n",
    "            try: h=float(h)\n",
    "            except: h=None\n",
    "            try: w=float(w)\n",
    "            except: w=None\n",
    "            try: b=float(b)\n",
    "            except: b=None\n",
    "            h,w,b = clip_plausible(h,w,b)\n",
    "            sc=0.0\n",
    "            if is_num(h): sc+=1.0\n",
    "            if is_num(w): sc+=1.0\n",
    "            if is_num(h) and is_num(w):\n",
    "                b2 = recompute_bmi(h,w)\n",
    "                if is_num(b2): sc += 0.7\n",
    "                if is_num(b) and is_num(b2) and abs(float(b)-float(b2))<=BMI_TOL: sc += 0.4\n",
    "            elif is_num(b):\n",
    "                sc += 0.2\n",
    "            if sc>best[\"score\"]: best={\"h\":h,\"w\":w,\"b\":b,\"score\":sc}\n",
    "        if is_num(best[\"h\"]) and is_num(best[\"w\"]): break\n",
    "    H = round(float(best[\"h\"]),2) if is_num(best[\"h\"]) else None\n",
    "    W = round(float(best[\"w\"]),1) if is_num(best[\"w\"]) else None\n",
    "    B_from = recompute_bmi(H,W) if (is_num(H) and is_num(W)) else None\n",
    "    B = B_from if is_num(B_from) else (round(float(best[\"b\"]),2) if is_num(best[\"b\"]) else None)\n",
    "    return H,W,B_from,B\n",
    "\n",
    "def run_fewshot(note_text: str, system_text: str, llm_generate, apply_chat_template):\n",
    "    \"\"\"\n",
    "    Igual que triplet, pero el prompt incluye ejemplos Q/A cuando no hay chat_template.\n",
    "    \"\"\"\n",
    "    wins = sorted([(s,c) for s,c in window_iter(note_text)], key=lambda x: int(not has_unit_token(x[1])))\n",
    "    best={\"h\":None,\"w\":None,\"b\":None,\"score\":-1e9}\n",
    "    for _, chunk in wins[:N_WINDOWS_MAX]:\n",
    "        # Construcción del prompt con ejemplos\n",
    "        msgs = [{\"role\":\"system\",\"content\":system_text}]\n",
    "        for ex_in, ex_out in FEW_SHOTS:\n",
    "            msgs += [{\"role\":\"user\",\"content\":\"NOTE WINDOW:\\n\"+ex_in+\"\\n\\nJSON ONLY\"},\n",
    "                     {\"role\":\"assistant\",\"content\":ex_out}]\n",
    "        msgs += [{\"role\":\"user\",\"content\":\"NOTE WINDOW:\\n\"+chunk+\"\\n\\nJSON ONLY\"}]\n",
    "\n",
    "        # Si hay plantilla, úsala; si no, ensamblamos Q/A manual con Output:\n",
    "        prompt = None\n",
    "        try:\n",
    "            prompt = apply_chat_template(system_text, \"NOTE WINDOW:\\n\"+chunk+\"\\n\\nJSON ONLY\")  # fallback mínimo\n",
    "            # Si queremos forzar los ejemplos en el prompt incluso con plantilla ausente:\n",
    "            if \"Output:\" in prompt:\n",
    "                pass\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        if prompt is None or (\"Output:\" not in prompt):\n",
    "            # Ensamble few-shot Q/A manual\n",
    "            few = []\n",
    "            for ex_in, ex_out in FEW_SHOTS:\n",
    "                few.append(\n",
    "                    \"Input:\\n\"+ex_in+\"\\nOutput:\\n\"+ex_out+\"\\n\"\n",
    "                )\n",
    "            few_block = \"\\n\".join(few)\n",
    "            prompt = (\n",
    "                \"Task:\\n\"+system_text.strip()+\"\\n\\n\"+\n",
    "                \"Examples:\\n\"+few_block+\"\\n\"+\n",
    "                \"Input:\\n\"+(\"NOTE WINDOW:\\n\"+chunk+\"\\n\\nJSON ONLY\")+\"\\n\"+\n",
    "                \"Output:\\n\"\n",
    "            )\n",
    "\n",
    "        # Generar y puntuar\n",
    "        for _ in range(ATTEMPTS_PER_WIN):\n",
    "            obj = safe_json(llm_generate(prompt))\n",
    "            if obj is None: continue\n",
    "            h,w,b = obj.get(\"height_m\"), obj.get(\"weight_kg\"), obj.get(\"bmi\")\n",
    "            try: h=float(h)\n",
    "            except: h=None\n",
    "            try: w=float(w)\n",
    "            except: w=None\n",
    "            try: b=float(b)\n",
    "            except: b=None\n",
    "            h,w,b = clip_plausible(h,w,b)\n",
    "            sc=0.0\n",
    "            if is_num(h): sc+=1.0\n",
    "            if is_num(w): sc+=1.0\n",
    "            if is_num(h) and is_num(w):\n",
    "                b2 = recompute_bmi(h,w)\n",
    "                if is_num(b2): sc += 0.7\n",
    "                if is_num(b) and is_num(b2) and abs(float(b)-float(b2))<=BMI_TOL: sc += 0.4\n",
    "            elif is_num(b):\n",
    "                sc += 0.2\n",
    "            if sc>best[\"score\"]: best={\"h\":h,\"w\":w,\"b\":b,\"score\":sc}\n",
    "        if is_num(best[\"h\"]) and is_num(best[\"w\"]): break\n",
    "\n",
    "    H = round(float(best[\"h\"]),2) if is_num(best[\"h\"]) else None\n",
    "    W = round(float(best[\"w\"]),1) if is_num(best[\"w\"]) else None\n",
    "    B_from = recompute_bmi(H,W) if (is_num(H) and is_num(W)) else None\n",
    "    B = B_from if is_num(B_from) else (round(float(best[\"b\"]),2) if is_num(best[\"b\"]) else None)\n",
    "    return H,W,B_from,B\n",
    "\n",
    "#  Cadena (SPAN -> NORM -> JUEZ)\n",
    "def chat_prompt(apply_chat_template, system, user):\n",
    "    return apply_chat_template(system, user)\n",
    "\n",
    "def run_chain_on_window(window_text: str, llm_generate, apply_chat_template):\n",
    "    SYS_SPAN = (\n",
    "        \"You are a clinical span finder. From the NOTE WINDOW, pick ONLY the earliest sentence that \"\n",
    "        \"contains tokens/units for height or weight or BMI, and return STRICT JSON:\\n\"\n",
    "        \"{ \\\"sentence\\\": <string>, \\\"height_span\\\": <string|null>, \\\"weight_span\\\": <string|null>, \\\"bmi_span\\\": <string|null> }\\n\"\n",
    "        \"Spans must be exact substrings and include units when applicable. JSON only.\"\n",
    "    )\n",
    "    SYS_NORM = (\n",
    "        \"You are a clinical normalizer and calculator. Given the chosen sentence and spans, return STRICT JSON:\\n\"\n",
    "        \"{ \\\"height_m\\\": <float|null>, \\\"weight_kg\\\": <float|null>, \\\"bmi\\\": <float|null>, \"\n",
    "        \"\\\"bmi_source\\\": <\\\"from_text\\\"|\\\"from_hw\\\"|null>, \\\"check\\\": <\\\"ok\\\"|\\\"mismatch\\\"|\\\"insufficient\\\"> }\\n\"\n",
    "        \"Normalize units; if both H & W exist, COMPUTE bmi=kg/(m^2) (2 decimals). Prefer computed BMI if conflicting.\"\n",
    "    )\n",
    "    span_raw = llm_generate(chat_prompt(apply_chat_template, SYS_SPAN, f\"NOTE WINDOW:\\n{window_text}\\n\\nJSON ONLY\"))\n",
    "    span_obj = safe_json(span_raw) or {}\n",
    "    norm_user = json.dumps({\n",
    "        \"sentence\": span_obj.get(\"sentence\",\"\"),\n",
    "        \"height_span\": span_obj.get(\"height_span\"),\n",
    "        \"weight_span\": span_obj.get(\"weight_span\"),\n",
    "        \"bmi_span\": span_obj.get(\"bmi_span\")\n",
    "    }, ensure_ascii=False)\n",
    "    norm_raw = llm_generate(chat_prompt(apply_chat_template, SYS_NORM, norm_user + \"\\n\\nJSON ONLY\"))\n",
    "    norm_obj = safe_json(norm_raw) or {}\n",
    "    return {\n",
    "        \"sentence\": span_obj.get(\"sentence\",\"\"),\n",
    "        \"height_m\": norm_obj.get(\"height_m\"),\n",
    "        \"weight_kg\": norm_obj.get(\"weight_kg\"),\n",
    "        \"bmi\": norm_obj.get(\"bmi\"),\n",
    "        \"bmi_source\": norm_obj.get(\"bmi_source\"),\n",
    "        \"check\": norm_obj.get(\"check\")\n",
    "    }\n",
    "\n",
    "def run_chain_on_note(note_text: str, llm_generate, apply_chat_template, attempts_per_win=2, n_windows_max=6):\n",
    "    cands=[]\n",
    "    for _, chunk in list(window_iter(note_text))[:n_windows_max]:\n",
    "        for _ in range(attempts_per_win):\n",
    "            c = run_chain_on_window(chunk, llm_generate, apply_chat_template)\n",
    "            if isinstance(c, dict): cands.append(c)\n",
    "    SYS_JUDGE = (\n",
    "        \"You are a strict clinical judge. You will receive a list of candidate JSON objects each with \"\n",
    "        \"height_m, weight_kg, bmi, bmi_source, check, and the chosen sentence.\\n\"\n",
    "        \"Pick the single BEST candidate and return STRICT JSON with the same fields. Prefer check=\\\"ok\\\"; \"\n",
    "        \"ties → same sentence H/W with explicit units; ties → clearer SI units.\"\n",
    "    )\n",
    "    judge_user = json.dumps({\"candidates\": cands}, ensure_ascii=False)\n",
    "    judge_raw = llm_generate(chat_prompt(apply_chat_template, SYS_JUDGE, judge_user + \"\\n\\nJSON ONLY\"))\n",
    "    jud = safe_json(judge_raw) or {}\n",
    "    def to_float(x):\n",
    "        try: return float(x)\n",
    "        except: return None\n",
    "    return {\n",
    "        \"height_m_pred\": to_float(jud.get(\"height_m\")),\n",
    "        \"weight_kg_pred\": to_float(jud.get(\"weight_kg\")),\n",
    "        \"BMI_pred_raw\":   to_float(jud.get(\"bmi\")),\n",
    "        \"bmi_source\":     jud.get(\"bmi_source\"),\n",
    "        \"check\":          jud.get(\"check\")\n",
    "    }\n",
    "\n",
    "#  Orquestación por modelo \n",
    "notes_run = pd.read_csv(SAMPLE_OUT, dtype={\"patient_id\": str})\n",
    "notes_run[\"patient_id\"] = notes_run[\"patient_id\"].astype(str).str.strip()\n",
    "\n",
    "for ALT_MODEL_ID in args.models:\n",
    "    validate_model_or_fail(ALT_MODEL_ID, token=args.hf_token)\n",
    "    print(\"\\n\"+\"=\"*88)\n",
    "    print(f\" Ejecutando experimento para: {ALT_MODEL_ID}\")\n",
    "    print(\"=\"*88)\n",
    "\n",
    "    tokenizer, llm_generate, apply_chat_template = get_ov_model_and_tokenizer(ALT_MODEL_ID, args.device_pref, token=args.hf_token)\n",
    "\n",
    "    ALL_PRED_PATHS = []\n",
    "\n",
    "    def save_csv(rows, out_csv):\n",
    "        Path(out_csv).parent.mkdir(parents=True, exist_ok=True)\n",
    "        pd.DataFrame(rows).to_csv(out_csv, index=False)\n",
    "        print(f\" Guardado: {out_csv}\")\n",
    "        ALL_PRED_PATHS.append(out_csv)\n",
    "\n",
    "    #  V1: SIMPLE \n",
    "    rows=[]\n",
    "    for _, r in tqdm(notes_run.iterrows(), total=len(notes_run), desc=\"Inferencia v1_simple\"):\n",
    "        pid, note = r[\"patient_id\"], str(r[\"patient\"])\n",
    "        H,W,B_from,B = run_triplet(note, SYSTEM_SIMPLE, llm_generate, apply_chat_template)\n",
    "        rows.append({\n",
    "            \"patient_id\": pid,\n",
    "            \"height_m_pred\": H if H is not None else np.nan,\n",
    "            \"weight_kg_pred\": W if W is not None else np.nan,\n",
    "            \"BMI_from_pred_hw\": B_from if B_from is not None else np.nan,\n",
    "            \"BMI_pred_raw\": B if B is not None else np.nan,\n",
    "            \"note_len\": len(note),\n",
    "            \"prompt_id\": \"v1_simple\",\n",
    "            \"model_used\": ALT_MODEL_ID\n",
    "        })\n",
    "    save_csv(rows, f\"{OUT_DIR}/pred_v1_simple_{ALT_MODEL_ID.split('/')[-1]}_n{len(notes_run)}.csv\")\n",
    "\n",
    "    #  V2: ESTRICTO \n",
    "    rows=[]\n",
    "    for _, r in tqdm(notes_run.iterrows(), total=len(notes_run), desc=\"Inferencia v2_estricto\"):\n",
    "        pid, note = r[\"patient_id\"], str(r[\"patient\"])\n",
    "        H,W,B_from,B = run_triplet(note, SYSTEM_STRICT, llm_generate, apply_chat_template)\n",
    "        rows.append({\n",
    "            \"patient_id\": pid,\n",
    "            \"height_m_pred\": H if H is not None else np.nan,\n",
    "            \"weight_kg_pred\": W if W is not None else np.nan,\n",
    "            \"BMI_from_pred_hw\": B_from if B_from is not None else np.nan,\n",
    "            \"BMI_pred_raw\": B if B is not None else np.nan,\n",
    "            \"note_len\": len(note),\n",
    "            \"prompt_id\": \"v2_estricto\",\n",
    "            \"model_used\": ALT_MODEL_ID\n",
    "        })\n",
    "    save_csv(rows, f\"{OUT_DIR}/pred_v2_estricto_{ALT_MODEL_ID.split('/')[-1]}_n{len(notes_run)}.csv\")\n",
    "\n",
    "    #  V3: FEW-SHOT \n",
    "    rows=[]\n",
    "    for _, r in tqdm(notes_run.iterrows(), total=len(notes_run), desc=\"Inferencia v3_fewshot\"):\n",
    "        pid, note = r[\"patient_id\"], str(r[\"patient\"])\n",
    "        H,W,B_from,B = run_fewshot(note, SYSTEM_FEWSHOT, llm_generate, apply_chat_template)\n",
    "        rows.append({\n",
    "            \"patient_id\": pid,\n",
    "            \"height_m_pred\": H if H is not None else np.nan,\n",
    "            \"weight_kg_pred\": W if W is not None else np.nan,\n",
    "            \"BMI_from_pred_hw\": B_from if B_from is not None else np.nan,\n",
    "            \"BMI_pred_raw\": B if B is not None else np.nan,\n",
    "            \"note_len\": len(note),\n",
    "            \"prompt_id\": \"v3_fewshot\",\n",
    "            \"model_used\": ALT_MODEL_ID\n",
    "        })\n",
    "    save_csv(rows, f\"{OUT_DIR}/pred_v3_fewshot_{ALT_MODEL_ID.split('/')[-1]}_n{len(notes_run)}.csv\")\n",
    "\n",
    "    #  V4: ENCADENADO (SPAN → NORM → JUEZ) \n",
    "    rows=[]\n",
    "    for _, r in tqdm(notes_run.iterrows(), total=len(notes_run), desc=\"Inferencia v4_encadenado\"):\n",
    "        pid, note = r[\"patient_id\"], str(r[\"patient\"])\n",
    "        out = run_chain_on_note(note, llm_generate, apply_chat_template, attempts_per_win=2, n_windows_max=6)\n",
    "        rows.append({\n",
    "            \"patient_id\": pid,\n",
    "            \"note_len\": len(note),\n",
    "            \"prompt_id\": \"v4_encadenado\",\n",
    "            \"model_used\": ALT_MODEL_ID,\n",
    "            \"height_m_pred\": (out[\"height_m_pred\"] if out[\"height_m_pred\"] is not None else np.nan),\n",
    "            \"weight_kg_pred\": (out[\"weight_kg_pred\"] if out[\"weight_kg_pred\"] is not None else np.nan),\n",
    "            \"BMI_pred_raw\":   (out[\"BMI_pred_raw\"] if out[\"BMI_pred_raw\"] is not None else np.nan),\n",
    "            \"BMI_from_pred_hw\": np.nan,\n",
    "            \"bmi_source\": out.get(\"bmi_source\"),\n",
    "            \"check\": out.get(\"check\")\n",
    "        })\n",
    "    save_csv(rows, f\"{OUT_DIR}/pred_v4_encadenado_{ALT_MODEL_ID.split('/')[-1]}_n{len(notes_run)}.csv\")\n",
    "\n",
    "    print(\"\\n Ficheros de predicción generados para\", ALT_MODEL_ID)\n",
    "    for p in ALL_PRED_PATHS: print(\" -\", p)\n",
    "\n",
    "print(\"\\n Fin.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "942c5e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Instalando dependencias que faltan:\n",
      "  - optimum-intel[openvino]==1.26.0\n",
      " Dependencias listas. Reiniciando imports...\n",
      " Modelo válido en HF: microsoft/BioGPT-Large  (sha: c6a5136)\n",
      "Notas totales: 50 | GT: 50 | Intersección: 50 | Usadas: 20\n",
      "Muestra guardada: C:\\Users\\hered\\Desktop\\TFM\\TFM\\TFM2\\outputs\\notes_sample_20.csv\n",
      "\n",
      "Cargando tokenizer: microsoft/BioGPT-Large\n",
      "Cargando modelo OpenVINO en GPU (exportando si es necesario)...\n",
      "INFO:nncf:Statistics of the bitwidth distribution:\n",
      "+---------------------------+-----------------------------+----------------------------------------+\n",
      "| Weight compression mode   | % all parameters (layers)   | % ratio-defining parameters (layers)   |\n",
      "+===========================+=============================+========================================+\n",
      "| int8_asym                 | 100% (290 / 290)            | 100% (290 / 290)                       |\n",
      "+---------------------------+-----------------------------+----------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Modelo operativo en GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencia v1_simple: 100%|██████████| 20/20 [53:27<00:00, 160.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Guardado: C:\\Users\\hered\\Desktop\\TFM\\TFM\\TFM2\\outputs/pred_v1_simple_BioGPT-Large_n20.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencia v2_estricto: 100%|██████████| 20/20 [52:59<00:00, 158.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Guardado: C:\\Users\\hered\\Desktop\\TFM\\TFM\\TFM2\\outputs/pred_v2_estricto_BioGPT-Large_n20.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencia v3_fewshot: 100%|██████████| 20/20 [56:26<00:00, 169.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Guardado: C:\\Users\\hered\\Desktop\\TFM\\TFM\\TFM2\\outputs/pred_v3_fewshot_BioGPT-Large_n20.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencia v4_encadenado: 100%|██████████| 20/20 [1:16:23<00:00, 229.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Guardado: C:\\Users\\hered\\Desktop\\TFM\\TFM\\TFM2\\outputs/pred_v4_encadenado_BioGPT-Large_n20.csv\n",
      "\n",
      " Ficheros de predicción generados para microsoft/BioGPT-Large\n",
      " - C:\\Users\\hered\\Desktop\\TFM\\TFM\\TFM2\\outputs/pred_v1_simple_BioGPT-Large_n20.csv\n",
      " - C:\\Users\\hered\\Desktop\\TFM\\TFM\\TFM2\\outputs/pred_v2_estricto_BioGPT-Large_n20.csv\n",
      " - C:\\Users\\hered\\Desktop\\TFM\\TFM\\TFM2\\outputs/pred_v3_fewshot_BioGPT-Large_n20.csv\n",
      " - C:\\Users\\hered\\Desktop\\TFM\\TFM\\TFM2\\outputs/pred_v4_encadenado_BioGPT-Large_n20.csv\n",
      "\n",
      " Fin.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# inferencia_local_openvino_biogpt_large.py\n",
    "#  INFERENCIA (4 VARIANTES DE PROMPT) — CLÍNICO (LOCAL, OpenVINO) \n",
    "# Modelo: microsoft/BioGPT-Large\n",
    "# Backend: OpenVINO (Intel GPU si está; fallback CPU). Guarda CSVs localmente.\n",
    "\n",
    "import os, sys, json, random, warnings, subprocess\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#  AUTO-INSTALACIÓN DE DEPENDENCIAS FALTANTES \n",
    "REQUIRED = [\n",
    "    \"numpy<2.1\",\n",
    "    \"transformers==4.46.2\",\n",
    "    \"optimum-intel[openvino]==1.26.0\",\n",
    "    \"openvino>=2025.1.0\",\n",
    "    \"accelerate>=0.34\",\n",
    "    \"huggingface_hub>=0.24\",\n",
    "    \"sacremoses\",            # <- necesario para BioGPT tokenizer\n",
    "    \"tqdm\",\n",
    "    \"pandas\"\n",
    "]\n",
    "\n",
    "def ensure_packages(pkgs):\n",
    "    def _is_installed(pkg_spec: str) -> bool:\n",
    "        # Comprobación simple por import principal\n",
    "        name = pkg_spec.split(\"==\")[0].split(\">=\")[0].split(\"<\")[0]\n",
    "        try:\n",
    "            __import__(name.replace(\"-\", \"_\"))\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    need = [p for p in pkgs if not _is_installed(p)]\n",
    "    if need:\n",
    "        print(\" Instalando dependencias que faltan:\\n  - \" + \"\\n  - \".join(need))\n",
    "        cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\"] + need\n",
    "        subprocess.check_call(cmd)\n",
    "        print(\" Dependencias listas. Reiniciando imports...\")\n",
    "ensure_packages(REQUIRED)\n",
    "\n",
    "#  Imports tras asegurar paquetes \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from optimum.intel.openvino import OVModelForCausalLM\n",
    "from huggingface_hub import HfApi\n",
    "try:\n",
    "    from huggingface_hub.utils import HfHubHTTPError\n",
    "except Exception:\n",
    "    try:\n",
    "        from huggingface_hub.utils._errors import HfHubHTTPError\n",
    "    except Exception:\n",
    "        HfHubHTTPError = Exception\n",
    "\n",
    "#  Config CLI (compatible con Jupyter/VSCode)\n",
    "def parse_args(argv=None):\n",
    "    ap = argparse.ArgumentParser(add_help=True)\n",
    "    ap.add_argument(\"--notes\", default=r\"C:\\Users\\hered\\Desktop\\TFM\\TFM\\TFM2\\sample_notes_imc.csv\", help=\"CSV con las 20 notas\")\n",
    "    ap.add_argument(\"--gt\",    default=r\"C:\\Users\\hered\\Desktop\\TFM\\TFM\\TFM2\\gt_imc_final.csv\",     help=\"CSV con GT\")\n",
    "    ap.add_argument(\"--outdir\",default=r\"C:\\Users\\hered\\Desktop\\TFM\\TFM\\TFM2\\outputs\",              help=\"Directorio de salida\")\n",
    "    ap.add_argument(\"--n\", type=int, default=20, help=\"Número de notas a usar\")\n",
    "    ap.add_argument(\"--device_pref\", default=\"GPU\", choices=[\"GPU\",\"CPU\"], help=\"Preferencia de dispositivo OpenVINO\")\n",
    "    ap.add_argument(\"--model\", default=\"microsoft/BioGPT-Large\", help=\"ID del modelo clínico en HF\")\n",
    "    ap.add_argument(\"--hf_token\", default=os.getenv(\"HUGGINGFACE_HUB_TOKEN\", None), help=\"Token HF si repo es privado/gated\")\n",
    "    ap.add_argument(\"--seed\", type=int, default=7)\n",
    "    ap.add_argument(\"--limit_windows\", type=int, default=6)\n",
    "    ap.add_argument(\"--attempts_per_window\", type=int, default=3)\n",
    "    # En Jupyter/VSCode, ignora args de ipykernel\n",
    "    if argv is None:\n",
    "        argv = [] if \"ipykernel\" in sys.modules else None\n",
    "    args, _ = ap.parse_known_args(argv)\n",
    "    return args\n",
    "\n",
    "args = parse_args()\n",
    "random.seed(args.seed); np.random.seed(args.seed)\n",
    "\n",
    "# Rutas fijas (puedes cambiarlas si quieres)\n",
    "PATH_NOTES = args.notes\n",
    "PATH_GT    = args.gt\n",
    "OUT_DIR    = args.outdir\n",
    "N_NOTES    = args.n\n",
    "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "SAMPLE_OUT = str(Path(OUT_DIR) / f\"notes_sample_{N_NOTES}.csv\")\n",
    "\n",
    "# Helpers HF\n",
    "def validate_model_or_fail(repo_id: str, token: str | None = None):\n",
    "    api = HfApi()\n",
    "    try:\n",
    "        info = api.model_info(repo_id, token=token)\n",
    "        if not info:\n",
    "            raise SystemExit(f\" No se pudo obtener info del repo '{repo_id}'.\")\n",
    "        print(f\" Modelo válido en HF: {repo_id}  (sha: {getattr(info, 'sha', 'n/a')[:7]})\")\n",
    "    except HfHubHTTPError as e:\n",
    "        code = getattr(e.response, \"status_code\", None)\n",
    "        if code == 404:\n",
    "            raise SystemExit(f\" Repo no encontrado en HF: {repo_id}\")\n",
    "        elif code == 401:\n",
    "            raise SystemExit(\n",
    "                f\" 401 Unauthorized para {repo_id}.\\n\"\n",
    "                f\"- Ejecuta `hf auth login` o pasa --hf_token.\\n\"\n",
    "                f\"- Si es ‘gated’, acepta términos en su página.\"\n",
    "            )\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "validate_model_or_fail(args.model, token=args.hf_token)\n",
    "\n",
    "#  Cargar datos \n",
    "assert Path(PATH_NOTES).exists(), f\"Falta {PATH_NOTES}\"\n",
    "assert Path(PATH_GT).exists(), f\"Falta {PATH_GT}\"\n",
    "\n",
    "def load_notes(csv_path):\n",
    "    df = pd.read_csv(csv_path, dtype={\"patient_id\": str})\n",
    "    cols = {c.lower().strip(): c for c in df.columns}\n",
    "    pid = next((cols[c] for c in [\"patient_id\",\"id\",\"pid\",\"subject_id\"] if c in cols), None)\n",
    "    txt = next((cols[c] for c in [\"patient\",\"note_text\",\"note\",\"text\"] if c in cols), None)\n",
    "    assert pid and txt, \"Necesito columnas ['patient_id', 'patient'/'note_text']\"\n",
    "    df = df.rename(columns={pid:\"patient_id\", txt:\"patient\"})[[\"patient_id\",\"patient\"]]\n",
    "    df[\"patient_id\"] = df[\"patient_id\"].astype(str).str.strip()\n",
    "    df[\"patient\"]    = df[\"patient\"].astype(str)\n",
    "    return df\n",
    "\n",
    "notes_full = load_notes(PATH_NOTES)\n",
    "gt = pd.read_csv(PATH_GT, dtype={\"patient_id\": str})\n",
    "gt[\"patient_id\"] = gt[\"patient_id\"].astype(str).str.strip()\n",
    "for c in [\"height_m_true\",\"weight_kg_true\",\"BMI_true\"]:\n",
    "    if c in gt.columns:\n",
    "        gt[c] = pd.to_numeric(gt[c], errors=\"coerce\")\n",
    "gt[\"bmi_explicit_in_note\"] = gt[\"bmi_explicit_in_note\"].astype(bool) if \"bmi_explicit_in_note\" in gt.columns else False\n",
    "\n",
    "inter = notes_full.merge(gt[[\"patient_id\"]], on=\"patient_id\", how=\"inner\")\n",
    "assert len(inter) >= N_NOTES, f\"No hay suficientes IDs comunes para N={N_NOTES}.\"\n",
    "TXT_COL = \"patient\" if \"patient\" in inter.columns else (\"note_text\" if \"note_text\" in inter.columns else None)\n",
    "assert TXT_COL is not None, f\"Falta columna de texto en {PATH_NOTES}: {inter.columns.tolist()}\"\n",
    "notes_20 = (\n",
    "    inter[[\"patient_id\", TXT_COL]]\n",
    "    .rename(columns={TXT_COL: \"patient\"})\n",
    "    .drop_duplicates(\"patient_id\")\n",
    "    .head(N_NOTES)\n",
    "    .copy()\n",
    ")\n",
    "notes_20.to_csv(SAMPLE_OUT, index=False)\n",
    "print(f\"Notas totales: {len(notes_full)} | GT: {len(gt)} | Intersección: {len(inter)} | Usadas: {len(notes_20)}\")\n",
    "print(f\"✔ Muestra guardada: {SAMPLE_OUT}\")\n",
    "\n",
    "#  Modelo: OpenVINO (GPU Intel si hay; si falla → CPU) \n",
    "def get_ov_model_and_tokenizer(model_id: str, device_pref: str = \"GPU\", token: str | None = None):\n",
    "    \"\"\"\n",
    "    Carga tokenizer y modelo OpenVINO. Intenta GPU Intel; si falla, cae a CPU.\n",
    "    Devuelve: (tokenizer, llm_generate, apply_chat_template)\n",
    "    \"\"\"\n",
    "    print(f\"\\nCargando tokenizer: {model_id}\")\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True, token=token)\n",
    "    if tok.pad_token_id is None and tok.eos_token_id is not None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "\n",
    "    device = \"GPU\" if device_pref.upper() == \"GPU\" else \"CPU\"\n",
    "    print(f\"Cargando modelo OpenVINO en {device} (exportando si es necesario)...\")\n",
    "    ov_model = OVModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        export=True,\n",
    "        device=device,\n",
    "        compile=True,\n",
    "        trust_remote_code=True,\n",
    "        ov_config={\"CACHE_DIR\": str(Path(OUT_DIR) / \"ov_cache\")},\n",
    "        token=token,\n",
    "    )\n",
    "    if getattr(ov_model.config, \"pad_token_id\", None) is None and tok.eos_token_id is not None:\n",
    "        ov_model.config.pad_token_id = tok.eos_token_id\n",
    "\n",
    "    # Probar inferencia mínima; si falla en GPU, reintentar en CPU.\n",
    "    try:\n",
    "        test_ids = tok(\"ok\", return_tensors=\"pt\").input_ids\n",
    "        _ = ov_model.generate(test_ids, max_new_tokens=1)\n",
    "        print(f\" Modelo operativo en {device}\")\n",
    "    except Exception as e:\n",
    "        if device == \"GPU\":\n",
    "            print(f\" Falló en GPU ({e}). Reintentando en CPU…\")\n",
    "            ov_model = OVModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                export=True,\n",
    "                device=\"CPU\",\n",
    "                compile=True,\n",
    "                trust_remote_code=True,\n",
    "                ov_config={\"CACHE_DIR\": str(Path(OUT_DIR) / \"ov_cache\")},\n",
    "                token=token,\n",
    "            )\n",
    "            print(\" Modelo operativo en CPU\")\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    def llm_generate(prompt: str, max_new=160, temperature=0.8, top_p=0.95, do_sample=True):\n",
    "        # Algo más \"creativo\" para evitar respuestas vacías en modelos no-instruct\n",
    "        inputs = tok(prompt, return_tensors=\"pt\")\n",
    "        out_ids = ov_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new,\n",
    "            min_new_tokens=8,\n",
    "            do_sample=do_sample,\n",
    "            temperature=float(temperature),\n",
    "            top_p=float(top_p),\n",
    "            repetition_penalty=1.05,\n",
    "            eos_token_id=(tok.eos_token_id or ov_model.config.eos_token_id),\n",
    "            pad_token_id=(ov_model.config.pad_token_id or tok.eos_token_id),\n",
    "        )\n",
    "        return tok.decode(out_ids[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True).strip()\n",
    "\n",
    "    def apply_chat_template(system_text: str, user_text: str):\n",
    "        \"\"\"\n",
    "        Si el tokenizer trae chat_template, úsalo. Si no (p. ej., BioGPT),\n",
    "        construye un prompt Q/A simple.\n",
    "        \"\"\"\n",
    "        has_apply = hasattr(tok, \"apply_chat_template\")\n",
    "        has_template = bool(getattr(tok, \"chat_template\", None))\n",
    "        if has_apply and has_template:\n",
    "            return tok.apply_chat_template(\n",
    "                [\n",
    "                    {\"role\": \"system\", \"content\": system_text},\n",
    "                    {\"role\": \"user\",  \"content\": user_text},\n",
    "                ],\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "            )\n",
    "        # Fallback Q/A\n",
    "        return (\n",
    "            \"Task:\\n\" + system_text.strip() + \"\\n\\n\" +\n",
    "            \"Input:\\n\" + user_text.strip() + \"\\n\" +\n",
    "            \"Output:\\n\"\n",
    "        )\n",
    "\n",
    "    return tok, llm_generate, apply_chat_template\n",
    "\n",
    "tokenizer, llm_generate, apply_chat_template = get_ov_model_and_tokenizer(args.model, args.device_pref, token=args.hf_token)\n",
    "\n",
    "#  Helpers de ventana / plausibilidad \n",
    "WIN, STRIDE = 1100, 800\n",
    "UNIT_TOKENS = [\" cm\",\" m\",\"meter\",\"metre\",\"ft\",\" in\",\"inch\",\"kg\",\" lb\",\"lbs\",\"pound\",\" stone\",\" st\",\"bmi\",\"BMI\",\"weight\",\"height\"]\n",
    "\n",
    "def window_iter(text, win=WIN, stride=STRIDE):\n",
    "    t = str(text); n = len(t)\n",
    "    if n <= win:\n",
    "        yield 0, t; return\n",
    "    for i in range(0, n, stride):\n",
    "        yield i, t[i:i+win]\n",
    "        if i+win >= n: break\n",
    "\n",
    "def has_unit_token(s: str):\n",
    "    sl = (s or \"\").lower()\n",
    "    return any(tok.strip().lower() in sl for tok in UNIT_TOKENS)\n",
    "\n",
    "H_MIN, H_MAX = 1.2, 2.2\n",
    "W_MIN, W_MAX = 30, 300\n",
    "BMI_MIN, BMI_MAX = 10, 80\n",
    "BMI_TOL = 0.5\n",
    "\n",
    "def is_num(x):\n",
    "    try:\n",
    "        return np.isfinite(float(x))\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def clip_plausible(h, w, b):\n",
    "    try:\n",
    "        if is_num(h) and not (H_MIN <= float(h) <= H_MAX): h = None\n",
    "    except: h = None\n",
    "    try:\n",
    "        if is_num(w) and not (W_MIN <= float(w) <= W_MAX): w = None\n",
    "    except: w = None\n",
    "    try:\n",
    "        if is_num(b) and not (BMI_MIN <= float(b) <= BMI_MAX): b = None\n",
    "    except: b = None\n",
    "    return h, w, b\n",
    "\n",
    "def recompute_bmi(h, w):\n",
    "    try:\n",
    "        h = float(h); w = float(w)\n",
    "        if h > 0: return round(w/(h*h), 2)\n",
    "    except: pass\n",
    "    return None\n",
    "\n",
    "#  4 VARIANTES DE PROMPT \n",
    "# 1) SIMPLE (tripleta)\n",
    "SYSTEM_SIMPLE = (\n",
    "    \"You are a careful clinical extractor. From the GIVEN WINDOW ONLY, return STRICT JSON with normalized SI values:\\n\"\n",
    "    \"{ \\\"height_m\\\": <float|null>, \\\"weight_kg\\\": <float|null>, \\\"bmi\\\": <float|null> }\\n\"\n",
    "    \"Rules: Use ONLY numbers present; convert units to SI; if either height or weight is missing, bmi=null. Output JSON only.\"\n",
    ")\n",
    "\n",
    "# 2) ESTRICTO (unidades explícitas + verificación)\n",
    "SYSTEM_STRICT = (\n",
    "    \"You are a clinical extractor and verifier. From the GIVEN WINDOW ONLY, return STRICT JSON:\\n\"\n",
    "    \"{ \\\"height_m\\\": <float|null>, \\\"weight_kg\\\": <float|null>, \\\"bmi\\\": <float|null> }\\n\"\n",
    "    \"Rules:\\n\"\n",
    "    \"• Use ONLY numbers with explicit units (cm/m/ft-in → m; kg/lb/stone → kg).\\n\"\n",
    "    \"• Plausibility: 1.20 ≤ height_m ≤ 2.20, 30 ≤ weight_kg ≤ 300, 10 ≤ bmi ≤ 80.\\n\"\n",
    "    \"• If both H & W exist, compute bmi=kg/(m^2) (2 decimals) and prefer this over any conflicting BMI text.\\n\"\n",
    "    \"• If inconsistent, set bmi=null. Output JSON only.\"\n",
    ")\n",
    "\n",
    "# 3) FEW-SHOT (ejemplos)\n",
    "SYSTEM_FEWSHOT = SYSTEM_SIMPLE\n",
    "FEW_SHOTS = [\n",
    "    (\"A 60-year-old woman, height 165 cm and weight 68 kg.\",\n",
    "     \"{\\\"height_m\\\": 1.65, \\\"weight_kg\\\": 68.0, \\\"bmi\\\": 24.98}\"),\n",
    "    (\"Male, 1.80 m, 90 kg; BMI not explicitly stated in text.\",\n",
    "     \"{\\\"height_m\\\": 1.80, \\\"weight_kg\\\": 90.0, \\\"bmi\\\": 27.78}\"),\n",
    "    (\"Patient reports good energy. No numeric measurements present.\",\n",
    "     \"{\\\"height_m\\\": null, \\\"weight_kg\\\": null, \\\"bmi\\\": null}\"),\n",
    "]\n",
    "\n",
    "# 4) ENCADENADO (SPAN -> NORM -> JUEZ) — se define en funciones abajo\n",
    "\n",
    "#  Utilidades de generación / parsing n",
    "def safe_json(text: str):\n",
    "    if not text: return None\n",
    "    s = text.strip()\n",
    "    if s.startswith(\"```\"):\n",
    "        try:\n",
    "            s = s.split(\"```\", 1)[-1]\n",
    "            if \"```\" in s: s = s.split(\"```\",1)[0]\n",
    "        except: pass\n",
    "    a, b = s.find(\"{\"), s.rfind(\"}\")\n",
    "    if a!=-1 and b!=-1 and b>a: s = s[a:b+1]\n",
    "    s = s.replace(\"None\",\"null\").replace(\"NaN\",\"null\").replace(\",}\", \"}\")\n",
    "    try:\n",
    "        obj = json.loads(s)\n",
    "        return obj if isinstance(obj, dict) else None\n",
    "    except: return None\n",
    "\n",
    "def gen_prompt(apply_chat_template, system_text, window_text):\n",
    "    return apply_chat_template(system_text, \"NOTE WINDOW:\\n\"+window_text+\"\\n\\nJSON ONLY\")\n",
    "\n",
    "#  Ejecutores comunes\n",
    "ATTEMPTS_PER_WIN = args.attempts_per_window\n",
    "N_WINDOWS_MAX = args.limit_windows\n",
    "\n",
    "def run_triplet(note_text: str, system_text: str, llm_generate, apply_chat_template):\n",
    "    wins = sorted([(s,c) for s,c in window_iter(note_text)], key=lambda x: int(not has_unit_token(x[1])))\n",
    "    best={\"h\":None,\"w\":None,\"b\":None,\"score\":-1e9}\n",
    "    for _, chunk in wins[:N_WINDOWS_MAX]:\n",
    "        prompt = gen_prompt(apply_chat_template, system_text, chunk)\n",
    "        for _ in range(ATTEMPTS_PER_WIN):\n",
    "            obj = safe_json(llm_generate(prompt))\n",
    "            if obj is None: continue\n",
    "            h,w,b = obj.get(\"height_m\"), obj.get(\"weight_kg\"), obj.get(\"bmi\")\n",
    "            try: h=float(h)\n",
    "            except: h=None\n",
    "            try: w=float(w)\n",
    "            except: w=None\n",
    "            try: b=float(b)\n",
    "            except: b=None\n",
    "            h,w,b = clip_plausible(h,w,b)\n",
    "            sc=0.0\n",
    "            if is_num(h): sc+=1.0\n",
    "            if is_num(w): sc+=1.0\n",
    "            if is_num(h) and is_num(w):\n",
    "                b2 = recompute_bmi(h,w)\n",
    "                if is_num(b2): sc += 0.7\n",
    "                if is_num(b) and is_num(b2) and abs(float(b)-float(b2))<=BMI_TOL: sc += 0.4\n",
    "            elif is_num(b):\n",
    "                sc += 0.2\n",
    "            if sc>best[\"score\"]: best={\"h\":h,\"w\":w,\"b\":b,\"score\":sc}\n",
    "        if is_num(best[\"h\"]) and is_num(best[\"w\"]): break\n",
    "    H = round(float(best[\"h\"]),2) if is_num(best[\"h\"]) else None\n",
    "    W = round(float(best[\"w\"]),1) if is_num(best[\"w\"]) else None\n",
    "    B_from = recompute_bmi(H,W) if (is_num(H) and is_num(W)) else None\n",
    "    B = B_from if is_num(B_from) else (round(float(best[\"b\"]),2) if is_num(best[\"b\"]) else None)\n",
    "    return H,W,B_from,B\n",
    "\n",
    "def run_fewshot(note_text: str, system_text: str, llm_generate, apply_chat_template):\n",
    "    # Igual que triplet, pero con ejemplos cuando no hay chat_template\n",
    "    wins = sorted([(s,c) for s,c in window_iter(note_text)], key=lambda x: int(not has_unit_token(x[1])))\n",
    "    best={\"h\":None,\"w\":None,\"b\":None,\"score\":-1e9}\n",
    "    for _, chunk in wins[:N_WINDOWS_MAX]:\n",
    "        # Construir prompt con ejemplos (fallback estilo Q/A)\n",
    "        few = []\n",
    "        for ex_in, ex_out in FEW_SHOTS:\n",
    "            few.append(\"Input:\\n\"+ex_in+\"\\nOutput:\\n\"+ex_out+\"\\n\")\n",
    "        few_block = \"\\n\".join(few)\n",
    "        prompt = (\n",
    "            \"Task:\\n\"+system_text.strip()+\"\\n\\n\"+\n",
    "            \"Examples:\\n\"+few_block+\"\\n\"+\n",
    "            \"Input:\\nNOTE WINDOW:\\n\"+chunk+\"\\n\\nJSON ONLY\\n\"+\n",
    "            \"Output:\\n\"\n",
    "        )\n",
    "        for _ in range(ATTEMPTS_PER_WIN):\n",
    "            obj = safe_json(llm_generate(prompt))\n",
    "            if obj is None: continue\n",
    "            h,w,b = obj.get(\"height_m\"), obj.get(\"weight_kg\"), obj.get(\"bmi\")\n",
    "            try: h=float(h)\n",
    "            except: h=None\n",
    "            try: w=float(w)\n",
    "            except: w=None\n",
    "            try: b=float(b)\n",
    "            except: b=None\n",
    "            h,w,b = clip_plausible(h,w,b)\n",
    "            sc=0.0\n",
    "            if is_num(h): sc+=1.0\n",
    "            if is_num(w): sc+=1.0\n",
    "            if is_num(h) and is_num(w):\n",
    "                b2 = recompute_bmi(h,w)\n",
    "                if is_num(b2): sc += 0.7\n",
    "                if is_num(b) and is_num(b2) and abs(float(b)-float(b2))<=BMI_TOL: sc += 0.4\n",
    "            elif is_num(b):\n",
    "                sc += 0.2\n",
    "            if sc>best[\"score\"]: best={\"h\":h,\"w\":w,\"b\":b,\"score\":sc}\n",
    "        if is_num(best[\"h\"]) and is_num(best[\"w\"]): break\n",
    "    H = round(float(best[\"h\"]),2) if is_num(best[\"h\"]) else None\n",
    "    W = round(float(best[\"w\"]),1) if is_num(best[\"w\"]) else None\n",
    "    B_from = recompute_bmi(H,W) if (is_num(H) and is_num(W)) else None\n",
    "    B = B_from if is_num(B_from) else (round(float(best[\"b\"]),2) if is_num(best[\"b\"]) else None)\n",
    "    return H,W,B_from,B\n",
    "\n",
    "#  Cadena (SPAN -> NORM -> JUEZ)\n",
    "def chat_prompt(apply_chat_template, system, user):\n",
    "    return apply_chat_template(system, user)\n",
    "\n",
    "def run_chain_on_window(window_text: str, llm_generate, apply_chat_template):\n",
    "    SYS_SPAN = (\n",
    "        \"You are a clinical span finder. From the NOTE WINDOW, pick ONLY the earliest sentence that \"\n",
    "        \"contains tokens/units for height or weight or BMI, and return STRICT JSON:\\n\"\n",
    "        \"{ \\\"sentence\\\": <string>, \\\"height_span\\\": <string|null>, \\\"weight_span\\\": <string|null>, \\\"bmi_span\\\": <string|null> }\\n\"\n",
    "        \"Spans must be exact substrings and include units when applicable. JSON only.\"\n",
    "    )\n",
    "    SYS_NORM = (\n",
    "        \"You are a clinical normalizer and calculator. Given the chosen sentence and spans, return STRICT JSON:\\n\"\n",
    "        \"{ \\\"height_m\\\": <float|null>, \\\"weight_kg\\\": <float|null>, \\\"bmi\\\": <float|null>, \"\n",
    "        \"\\\"bmi_source\\\": <\\\"from_text\\\"|\\\"from_hw\\\"|null>, \\\"check\\\": <\\\"ok\\\"|\\\"mismatch\\\"|\\\"insufficient\\\"> }\\n\"\n",
    "        \"Normalize units; if both H & W exist, COMPUTE bmi=kg/(m^2) (2 decimals). Prefer computed BMI if conflicting.\"\n",
    "    )\n",
    "    span_raw = llm_generate(chat_prompt(apply_chat_template, SYS_SPAN, f\"NOTE WINDOW:\\n{window_text}\\n\\nJSON ONLY\"))\n",
    "    span_obj = safe_json(span_raw) or {}\n",
    "    norm_user = json.dumps({\n",
    "        \"sentence\": span_obj.get(\"sentence\",\"\"),\n",
    "        \"height_span\": span_obj.get(\"height_span\"),\n",
    "        \"weight_span\": span_obj.get(\"weight_span\"),\n",
    "        \"bmi_span\": span_obj.get(\"bmi_span\")\n",
    "    }, ensure_ascii=False)\n",
    "    norm_raw = llm_generate(chat_prompt(apply_chat_template, SYS_NORM, norm_user + \"\\n\\nJSON ONLY\"))\n",
    "    norm_obj = safe_json(norm_raw) or {}\n",
    "    return {\n",
    "        \"sentence\": span_obj.get(\"sentence\",\"\"),\n",
    "        \"height_m\": norm_obj.get(\"height_m\"),\n",
    "        \"weight_kg\": norm_obj.get(\"weight_kg\"),\n",
    "        \"bmi\": norm_obj.get(\"bmi\"),\n",
    "        \"bmi_source\": norm_obj.get(\"bmi_source\"),\n",
    "        \"check\": norm_obj.get(\"check\")\n",
    "    }\n",
    "\n",
    "def run_chain_on_note(note_text: str, llm_generate, apply_chat_template, attempts_per_win=2, n_windows_max=6):\n",
    "    cands=[]\n",
    "    for _, chunk in list(window_iter(note_text))[:n_windows_max]:\n",
    "        for _ in range(attempts_per_win):\n",
    "            c = run_chain_on_window(chunk, llm_generate, apply_chat_template)\n",
    "            if isinstance(c, dict): cands.append(c)\n",
    "    SYS_JUDGE = (\n",
    "        \"You are a strict clinical judge. You will receive a list of candidate JSON objects each with \"\n",
    "        \"height_m, weight_kg, bmi, bmi_source, check, and the chosen sentence.\\n\"\n",
    "        \"Pick the single BEST candidate and return STRICT JSON with the same fields. Prefer check=\\\"ok\\\"; \"\n",
    "        \"ties → same sentence H/W with explicit units; ties → clearer SI units.\"\n",
    "    )\n",
    "    judge_user = json.dumps({\"candidates\": cands}, ensure_ascii=False)\n",
    "    judge_raw = llm_generate(chat_prompt(apply_chat_template, SYS_JUDGE, judge_user + \"\\n\\nJSON ONLY\"))\n",
    "    jud = safe_json(judge_raw) or {}\n",
    "    def to_float(x):\n",
    "        try: return float(x)\n",
    "        except: return None\n",
    "    return {\n",
    "        \"height_m_pred\": to_float(jud.get(\"height_m\")),\n",
    "        \"weight_kg_pred\": to_float(jud.get(\"weight_kg\")),\n",
    "        \"BMI_pred_raw\":   to_float(jud.get(\"bmi\")),\n",
    "        \"bmi_source\":     jud.get(\"bmi_source\"),\n",
    "        \"check\":          jud.get(\"check\")\n",
    "    }\n",
    "\n",
    "#  Orquestación — 4 variantes \n",
    "notes_run = pd.read_csv(SAMPLE_OUT, dtype={\"patient_id\": str})\n",
    "notes_run[\"patient_id\"] = notes_run[\"patient_id\"].astype(str).str.strip()\n",
    "\n",
    "ALL_PRED_PATHS = []\n",
    "\n",
    "def save_csv(rows, out_csv):\n",
    "    Path(out_csv).parent.mkdir(parents=True, exist_ok=True)\n",
    "    pd.DataFrame(rows).to_csv(out_csv, index=False)\n",
    "    print(f\"Guardado: {out_csv}\")\n",
    "    ALL_PRED_PATHS.append(out_csv)\n",
    "\n",
    "# V1: SIMPLE\n",
    "rows=[]\n",
    "for _, r in tqdm(notes_run.iterrows(), total=len(notes_run), desc=\"Inferencia v1_simple\"):\n",
    "    pid, note = r[\"patient_id\"], str(r[\"patient\"])\n",
    "    H,W,B_from,B = run_triplet(note, SYSTEM_SIMPLE, llm_generate, apply_chat_template)\n",
    "    rows.append({\n",
    "        \"patient_id\": pid,\n",
    "        \"height_m_pred\": H if H is not None else np.nan,\n",
    "        \"weight_kg_pred\": W if W is not None else np.nan,\n",
    "        \"BMI_from_pred_hw\": B_from if B_from is not None else np.nan,\n",
    "        \"BMI_pred_raw\": B if B is not None else np.nan,\n",
    "        \"note_len\": len(note),\n",
    "        \"prompt_id\": \"v1_simple\",\n",
    "        \"model_used\": args.model\n",
    "    })\n",
    "save_csv(rows, f\"{OUT_DIR}/pred_v1_simple_{args.model.split('/')[-1]}_n{len(notes_run)}.csv\")\n",
    "\n",
    "# V2: ESTRICTO\n",
    "rows=[]\n",
    "for _, r in tqdm(notes_run.iterrows(), total=len(notes_run), desc=\"Inferencia v2_estricto\"):\n",
    "    pid, note = r[\"patient_id\"], str(r[\"patient\"])\n",
    "    H,W,B_from,B = run_triplet(note, SYSTEM_STRICT, llm_generate, apply_chat_template)\n",
    "    rows.append({\n",
    "        \"patient_id\": pid,\n",
    "        \"height_m_pred\": H if H is not None else np.nan,\n",
    "        \"weight_kg_pred\": W if W is not None else np.nan,\n",
    "        \"BMI_from_pred_hw\": B_from if B_from is not None else np.nan,\n",
    "        \"BMI_pred_raw\": B if B is not None else np.nan,\n",
    "        \"note_len\": len(note),\n",
    "        \"prompt_id\": \"v2_estricto\",\n",
    "        \"model_used\": args.model\n",
    "    })\n",
    "save_csv(rows, f\"{OUT_DIR}/pred_v2_estricto_{args.model.split('/')[-1]}_n{len(notes_run)}.csv\")\n",
    "\n",
    "# V3: FEW-SHOT\n",
    "rows=[]\n",
    "for _, r in tqdm(notes_run.iterrows(), total=len(notes_run), desc=\"Inferencia v3_fewshot\"):\n",
    "    pid, note = r[\"patient_id\"], str(r[\"patient\"])\n",
    "    H,W,B_from,B = run_fewshot(note, SYSTEM_FEWSHOT, llm_generate, apply_chat_template)\n",
    "    rows.append({\n",
    "        \"patient_id\": pid,\n",
    "        \"height_m_pred\": H if H is not None else np.nan,\n",
    "        \"weight_kg_pred\": W if W is not None else np.nan,\n",
    "        \"BMI_from_pred_hw\": B_from if B_from is not None else np.nan,\n",
    "        \"BMI_pred_raw\": B if B is not None else np.nan,\n",
    "        \"note_len\": len(note),\n",
    "        \"prompt_id\": \"v3_fewshot\",\n",
    "        \"model_used\": args.model\n",
    "    })\n",
    "save_csv(rows, f\"{OUT_DIR}/pred_v3_fewshot_{args.model.split('/')[-1]}_n{len(notes_run)}.csv\")\n",
    "\n",
    "# V4: ENCADENADO (SPAN → NORM → JUEZ)\n",
    "rows=[]\n",
    "for _, r in tqdm(notes_run.iterrows(), total=len(notes_run), desc=\"Inferencia v4_encadenado\"):\n",
    "    pid, note = r[\"patient_id\"], str(r[\"patient\"])\n",
    "    out = run_chain_on_note(note, llm_generate, apply_chat_template, attempts_per_win=2, n_windows_max=6)\n",
    "    rows.append({\n",
    "        \"patient_id\": pid,\n",
    "        \"note_len\": len(note),\n",
    "        \"prompt_id\": \"v4_encadenado\",\n",
    "        \"model_used\": args.model,\n",
    "        \"height_m_pred\": (out[\"height_m_pred\"] if out[\"height_m_pred\"] is not None else np.nan),\n",
    "        \"weight_kg_pred\": (out[\"weight_kg_pred\"] if out[\"weight_kg_pred\"] is not None else np.nan),\n",
    "        \"BMI_pred_raw\":   (out[\"BMI_pred_raw\"] if out[\"BMI_pred_raw\"] is not None else np.nan),\n",
    "        \"BMI_from_pred_hw\": np.nan,\n",
    "        \"bmi_source\": out.get(\"bmi_source\"),\n",
    "        \"check\": out.get(\"check\")\n",
    "    })\n",
    "save_csv(rows, f\"{OUT_DIR}/pred_v4_encadenado_{args.model.split('/')[-1]}_n{len(notes_run)}.csv\")\n",
    "\n",
    "print(\"\\n Ficheros de predicción generados para\", args.model)\n",
    "for p in ALL_PRED_PATHS: print(\" -\", p)\n",
    "print(\"\\nFin.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0ba121f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"DOSKEY\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n",
      "ERROR: Could not find a version that satisfies the requirement torch==2.1.0 (from versions: 2.2.0+cpu, 2.2.1+cpu, 2.2.2+cpu, 2.3.0+cpu, 2.3.1+cpu, 2.4.0+cpu, 2.4.1+cpu, 2.5.0+cpu, 2.5.1+cpu, 2.6.0+cpu, 2.7.0+cpu, 2.7.1+cpu, 2.8.0+cpu, 2.9.0+cpu)\n",
      "ERROR: No matching distribution found for torch==2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall torch torchvision torchaudio\n",
    "!pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ba311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ov_model_and_tokenizer(model_id: str, device_pref: str = \"GPU\", token: str | None = None):\n",
    "    print(f\"\\nCargando tokenizer: {model_id}\")\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True, token=token)\n",
    "    if tok.pad_token_id is None and tok.eos_token_id is not None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "\n",
    "    device = \"GPU\" if device_pref.upper() == \"GPU\" else \"CPU\"\n",
    "    print(f\"Cargando modelo OpenVINO en {device}...\")\n",
    "    \n",
    "    # Configuración para evitar problemas de tracing\n",
    "    ov_config_dict = {\n",
    "        \"CACHE_DIR\": str(Path(OUT_DIR) / \"ov_cache\"),\n",
    "        \"PERFORMANCE_HINT\": \"LATENCY\",\n",
    "        \"INFERENCE_PRECISION_HINT\": \"f32\"  # Forzar precisión float32\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Intentar cargar sin exportar primero\n",
    "        ov_model = OVModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            export=False,  # No exportar, solo cargar\n",
    "            device=device,\n",
    "            compile=False,  # No compilar inmediatamente\n",
    "            trust_remote_code=True,\n",
    "            ov_config=ov_config_dict,\n",
    "            token=token,\n",
    "        )\n",
    "    except Exception:\n",
    "        # Si falla, intentar con exportación pero con configuraciones más conservadoras\n",
    "        print(\"  Modelo no encontrado en formato OpenVINO. Exportando...\")\n",
    "        try:\n",
    "            ov_model = OVModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                export=True,\n",
    "                device=\"CPU\",  # Usar CPU para exportación que es más estable\n",
    "                compile=False,\n",
    "                trust_remote_code=True,\n",
    "                ov_config=ov_config_dict,\n",
    "                token=token,\n",
    "                # Parámetros específicos para evitar problemas de tracing\n",
    "                model_kwargs={\n",
    "                    \"torch_dtype\": torch.float32,\n",
    "                    \"low_cpu_mem_usage\": True\n",
    "                }\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\" Error en exportación: {e}\")\n",
    "            print(\" Intentando enfoque alternativo...\")\n",
    "            # Enfoque alternativo: cargar el modelo PyTorch primero y luego convertirlo\n",
    "            from transformers import AutoModelForCausalLM\n",
    "            import torch\n",
    "            \n",
    "            print(\"Cargando modelo PyTorch primero...\")\n",
    "            pt_model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                torch_dtype=torch.float32,\n",
    "                trust_remote_code=True,\n",
    "                token=token\n",
    "            )\n",
    "            \n",
    "            # Forzar el modelo a modo evaluación y float32\n",
    "            pt_model.eval()\n",
    "            pt_model = pt_model.to(torch.float32)\n",
    "            \n",
    "            print(\"Exportando desde modelo PyTorch cargado...\")\n",
    "            ov_model = OVModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                export=True,\n",
    "                device=\"CPU\",\n",
    "                compile=False,\n",
    "                trust_remote_code=True,\n",
    "                ov_config=ov_config_dict,\n",
    "                token=token,\n",
    "                model_kwargs={\"_from_torch\": True}\n",
    "            )\n",
    "\n",
    "    # Configurar token de padding si es necesario\n",
    "    if getattr(ov_model.config, \"pad_token_id\", None) is None and tok.eos_token_id is not None:\n",
    "        ov_model.config.pad_token_id = tok.eos_token_id\n",
    "\n",
    "    # Ahora compilar el modelo\n",
    "    print(\"🔧 Compilando modelo...\")\n",
    "    ov_model.compile()\n",
    "\n",
    "    # Probar inferencia mínima\n",
    "    try:\n",
    "        test_ids = tok(\"ok\", return_tensors=\"pt\").input_ids\n",
    "        # Usar configuración más conservadora para la prueba\n",
    "        _ = ov_model.generate(test_ids, max_new_tokens=1, do_sample=False)\n",
    "        print(f\" Modelo operativo en {device}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Advertencia en prueba de inferencia: {e}\")\n",
    "        # Continuar de todos modos\n",
    "\n",
    "    def llm_generate(prompt: str, max_new=180, temperature=0.9, top_p=0.95, do_sample=True):\n",
    "        inputs = tok(prompt, return_tensors=\"pt\")\n",
    "        \n",
    "        # Configuración más robusta para generación\n",
    "        generation_config = {\n",
    "            \"max_new_tokens\": max_new,\n",
    "            \"min_new_tokens\": 8,\n",
    "            \"do_sample\": do_sample,\n",
    "            \"temperature\": float(temperature),\n",
    "            \"top_p\": float(top_p),\n",
    "            \"repetition_penalty\": 1.05,\n",
    "            \"eos_token_id\": (tok.eos_token_id or getattr(ov_model.config, \"eos_token_id\", None)),\n",
    "            \"pad_token_id\": (getattr(ov_model.config, \"pad_token_id\", None) or tok.eos_token_id),\n",
    "        }\n",
    "        \n",
    "        # Para primeros intentos, usar configuración más conservadora\n",
    "        if not do_sample:\n",
    "            generation_config.update({\n",
    "                \"num_beams\": 1,\n",
    "                \"do_sample\": False\n",
    "            })\n",
    "        \n",
    "        try:\n",
    "            out_ids = ov_model.generate(**inputs, **generation_config)\n",
    "            return tok.decode(out_ids[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True).strip()\n",
    "        except Exception as e:\n",
    "            print(f\" Error en generación: {e}\")\n",
    "            # Fallback: intentar con configuración mínima\n",
    "            try:\n",
    "                out_ids = ov_model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=50,\n",
    "                    do_sample=False,\n",
    "                    num_beams=1\n",
    "                )\n",
    "                return tok.decode(out_ids[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True).strip()\n",
    "            except:\n",
    "                return \"\"\n",
    "\n",
    "    def apply_chat_template(system_text: str, user_text: str):\n",
    "        \"\"\"Aplicar plantilla de chat de manera segura\"\"\"\n",
    "        try:\n",
    "            has_apply = hasattr(tok, \"apply_chat_template\")\n",
    "            has_template = bool(getattr(tok, \"chat_template\", None))\n",
    "            if has_apply and has_template:\n",
    "                return tok.apply_chat_template(\n",
    "                    [\n",
    "                        {\"role\": \"system\", \"content\": system_text},\n",
    "                        {\"role\": \"user\",  \"content\": user_text},\n",
    "                    ],\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True,\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\" Error aplicando plantilla de chat: {e}\")\n",
    "        \n",
    "        # Fallback Q/A\n",
    "        return (\n",
    "            \"Task:\\n\" + system_text.strip() + \"\\n\\n\" +\n",
    "            \"Input:\\n\" + user_text.strip() + \"\\n\" +\n",
    "            \"Output:\\n\"\n",
    "        )\n",
    "\n",
    "    return tok, llm_generate, apply_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45967ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIRED = [\n",
    "    \"numpy<2.1\",\n",
    "    \"transformers==4.46.2\",\n",
    "    \"optimum-intel[openvino]==1.26.0\", \n",
    "    \"openvino>=2025.1.0\",\n",
    "    \"accelerate>=0.34\",\n",
    "    \"huggingface_hub>=0.24\",\n",
    "    \"sacremoses\",\n",
    "    \"sentencepiece\", \n",
    "    \"tiktoken\",\n",
    "    \"pandas\",\n",
    "    \"tqdm\",\n",
    "    \"torch==2.1.0\",  # Versión específica compatible\n",
    "    \"torchvision==0.16.0\",\n",
    "    \"torchaudio==2.1.0\"\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
