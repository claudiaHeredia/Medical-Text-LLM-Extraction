{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e172f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- RUTA A TU DATASET ---\n",
    "DATA_CSV = r\"C:/Users/hered/Desktop/TFM/TFM/data/PMC-Patients.csv\"\n",
    "OUT_DIR  = Path(r\"C:/Users/hered/Desktop/TFM/TFM/IMC2\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ================= Normalización de texto =================\n",
    "def norm_text(s: str) -> str:\n",
    "    s = (s or \"\")\n",
    "    s = s.replace(\"\\u2009\", \" \").replace(\"\\u00a0\", \" \").replace(\"\\u2011\",\"-\")\n",
    "    s = s.replace(\"\\r\", \" \").replace(\"\\t\", \" \")\n",
    "    # compactar espacios\n",
    "    s = re.sub(r\"[ \\u00a0]{2,}\", \" \", s)\n",
    "    return s\n",
    "\n",
    "# ================= Conversión de unidades =================\n",
    "def to_meters(val: float, unit: str, inches_extra: float|None=None):\n",
    "    u = (unit or \"\").strip().lower()\n",
    "    if u in (\"m\",\"meter\",\"meters\"):\n",
    "        return val if 0.3 <= val <= 3.0 else None\n",
    "    if u in (\"cm\",\"centimeter\",\"centimeters\"):\n",
    "        return val/100.0\n",
    "    if u in (\"ft\",\"foot\",\"feet\"):\n",
    "        inches = inches_extra or 0.0\n",
    "        return val*0.3048 + inches*0.0254\n",
    "    if u in (\"in\",\"inch\",\"inches\",\"\\\"\"):\n",
    "        return val*0.0254\n",
    "    return None\n",
    "\n",
    "def to_kg(val: float, unit: str, lbs_extra: float|None=None):\n",
    "    u = (unit or \"\").strip().lower()\n",
    "    if u in (\"kg\",\"kgs\",\"kilogram\",\"kilograms\"):\n",
    "        return val\n",
    "    if u in (\"lb\",\"lbs\",\"pound\",\"pounds\"):\n",
    "        return val*0.453592 + (lbs_extra or 0.0)*0.453592\n",
    "    if u in (\"st\",\"stone\",\"stones\"):\n",
    "        # 1 stone = 14 lb\n",
    "        total_lb = val*14 + (lbs_extra or 0.0)\n",
    "        return total_lb*0.453592\n",
    "    return None\n",
    "\n",
    "# ================= Plausibilidad =================\n",
    "def plausible_height_m(h):\n",
    "    return (h is not None) and (1.3 <= h <= 2.2)\n",
    "\n",
    "def plausible_weight_kg(w):\n",
    "    return (w is not None) and (30 <= w <= 250)\n",
    "\n",
    "# ================= Blacklist / Filtros de contexto =================\n",
    "blacklist_terms = {\n",
    "    # evita confundir \"mass\" (tumor) con \"body mass\"\n",
    "    \"mass\", \"lesion\", \"tumor\", \"nodule\", \"cyst\", \"defect\", \"stone\", \"specimen\",\n",
    "    # también palabras de laboratorio / fármacos\n",
    "    \"mg \", \"mcg\", \"ml \", \"dose\", \"dosage\",\n",
    "}\n",
    "\n",
    "def has_blacklist(snippet: str) -> bool:\n",
    "    t = (snippet or \"\").lower()\n",
    "    return any(b in t for b in blacklist_terms)\n",
    "\n",
    "# ================== REGEX Altura/Peso robustos ==================\n",
    "# -- Altura --\n",
    "# 1) formateos con pies y pulgadas: 5'7\", 5 ft 7 in, 5 feet 7 inches, 5′7″ (apóstrofos tipográficos)\n",
    "ft_in_1 = re.compile(r\"(?i)\\b(\\d)\\s*(?:ft|foot|feet|’|')\\s*(\\d{1,2})\\s*(?:in|inch|inches|\\\"|’’|”)?\\b\")\n",
    "# 2) solo pies (raro), o pulgadas sueltas\n",
    "ft_only = re.compile(r\"(?i)\\b(\\d)\\s*(?:ft|foot|feet)\\b\")\n",
    "in_only = re.compile(r\"(?i)\\b(\\d{2})\\s*(?:in|inch|inches|\\\"|’’|”)\\b\")\n",
    "# 3) metros o centímetros\n",
    "m_cm = re.compile(r\"(?i)\\b(\\d{0,1}\\.?\\d{1,3})\\s*(m|meter|meters)\\b|\\b(1\\d{2}|[5-9]\\d)\\s*(cm|centimeter|centimeters)\\b\")\n",
    "\n",
    "# -- Peso --\n",
    "# 1) kg, lbs, stone+lbs\n",
    "weight_main = re.compile(\n",
    "    r\"(?i)\\b(?:wt|weight|weighs?)\\s*[:=]?\\s*(\\d{2,3}(?:\\.\\d+)?)\\s*(kg|kgs|kilograms?|lb|lbs|pounds?|st|stone|stones)\\b\"\n",
    ")\n",
    "# 2) números con unidad sin palabra \"weight\": “70 kg”, “154 lb”\n",
    "weight_loose = re.compile(r\"(?i)\\b(\\d{2,3}(?:\\.\\d+)?)\\s*(kg|kgs|kilograms?|lb|lbs|pounds?|st|stone|stones)\\b\")\n",
    "# 3) stone + lb explícitos: “11 st 3 lb”\n",
    "stone_plus_lb = re.compile(r\"(?i)\\b(\\d{1,2})\\s*(?:st|stone|stones)\\s*(\\d{1,2})\\s*(?:lb|lbs|pounds?)\\b\")\n",
    "\n",
    "# Evitar capturar presión arterial como altura/peso: no usar números con \"/\" (BP 120/80) ni “mmHg”.\n",
    "bp_like = re.compile(r\"(?i)\\b(?:bp|blood\\s*pressure)\\b|mm\\s*hg|mmhg\")\n",
    "\n",
    "# Palabras clave que nos ayudan a elegir el par correcto\n",
    "context_words = re.compile(r\"(?i)\\b(height|stature|talla|weight|weighs?|wt|bmi|imc|vitals?|examination|admission|triage)\\b\")\n",
    "\n",
    "# ==================== Extracción por nota ====================\n",
    "def extract_heights(text: str):\n",
    "    t = norm_text(text)\n",
    "    spans = []\n",
    "\n",
    "    # 5'7\", 5 ft 7 in\n",
    "    for m in ft_in_1.finditer(t):\n",
    "        feet = float(m.group(1)); inches = float(m.group(2))\n",
    "        h_m = to_meters(feet, \"ft\", inches_extra=inches)\n",
    "        s = t[max(0, m.start()-40): m.end()+40]\n",
    "        if h_m is not None and not bp_like.search(s):\n",
    "            spans.append((\"height\", h_m, f\"{feet}ft {inches}in\", s, m.start(), m.end()))\n",
    "\n",
    "    # solo pies\n",
    "    for m in ft_only.finditer(t):\n",
    "        feet = float(m.group(1))\n",
    "        h_m = to_meters(feet, \"ft\")\n",
    "        s = t[max(0, m.start()-40): m.end()+40]\n",
    "        if h_m is not None and not bp_like.search(s):\n",
    "            spans.append((\"height\", h_m, f\"{feet}ft\", s, m.start(), m.end()))\n",
    "\n",
    "    # solo pulgadas\n",
    "    for m in in_only.finditer(t):\n",
    "        inches = float(m.group(1))\n",
    "        h_m = to_meters(inches, \"in\")\n",
    "        s = t[max(0, m.start()-40): m.end()+40]\n",
    "        if h_m is not None and not bp_like.search(s):\n",
    "            spans.append((\"height\", h_m, f\"{inches}in\", s, m.start(), m.end()))\n",
    "\n",
    "    # metros/centímetros (regex con 2 alternativas en uno)\n",
    "    for m in m_cm.finditer(t):\n",
    "        if m.group(1) and m.group(2):\n",
    "            # metros\n",
    "            val = float(m.group(1))\n",
    "            h_m = to_meters(val, m.group(2))\n",
    "            raw = f\"{val} {m.group(2)}\"\n",
    "            s = t[max(0, m.start()-40): m.end()+40]\n",
    "        else:\n",
    "            # centímetros\n",
    "            val = float(m.group(3))\n",
    "            h_m = to_meters(val, \"cm\")\n",
    "            raw = f\"{val} cm\"\n",
    "            s = t[max(0, m.start()-40): m.end()+40]\n",
    "        if h_m is not None and not bp_like.search(s):\n",
    "            spans.append((\"height\", h_m, raw, s, m.start(), m.end()))\n",
    "\n",
    "    # filtrar plausibles + sin blacklist\n",
    "    spans = [sp for sp in spans if plausible_height_m(sp[1]) and not has_blacklist(sp[3])]\n",
    "    return spans\n",
    "\n",
    "def extract_weights(text: str):\n",
    "    t = norm_text(text)\n",
    "    spans = []\n",
    "\n",
    "    # stone + lb (11 st 3 lb)\n",
    "    for m in stone_plus_lb.finditer(t):\n",
    "        st = float(m.group(1)); lb = float(m.group(2))\n",
    "        w_kg = to_kg(st, \"st\", lbs_extra=lb)\n",
    "        s = t[max(0, m.start()-40): m.end()+40]\n",
    "        if w_kg is not None:\n",
    "            spans.append((\"weight\", w_kg, f\"{st} st {lb} lb\", s, m.start(), m.end()))\n",
    "\n",
    "    # “weight 70 kg” o “70 kg”\n",
    "    for m in weight_main.finditer(t):\n",
    "        val = float(m.group(1)); unit = m.group(2)\n",
    "        w_kg = to_kg(val, unit)\n",
    "        s = t[max(0, m.start()-40): m.end()+40]\n",
    "        if w_kg is not None:\n",
    "            spans.append((\"weight\", w_kg, f\"{val} {unit}\", s, m.start(), m.end()))\n",
    "    for m in weight_loose.finditer(t):\n",
    "        val = float(m.group(1)); unit = m.group(2)\n",
    "        w_kg = to_kg(val, unit)\n",
    "        s = t[max(0, m.start()-40): m.end()+40]\n",
    "        if w_kg is not None:\n",
    "            spans.append((\"weight\", w_kg, f\"{val} {unit}\", s, m.start(), m.end()))\n",
    "\n",
    "    # filtrar plausibles + sin blacklist + no confundir con BP/mmHg\n",
    "    spans = [sp for sp in spans if plausible_weight_kg(sp[1]) and not has_blacklist(sp[3]) and not bp_like.search(sp[3])]\n",
    "    return spans\n",
    "\n",
    "# ==================== Scoring para elegir el mejor par ====================\n",
    "def pick_best_pair(h_spans, w_spans, text):\n",
    "    \"\"\"\n",
    "    Devuelve (h_span, w_span, score, reason)\n",
    "    Donde cada span: (kind, value, raw, snippet, start, end)\n",
    "    \"\"\"\n",
    "    if not h_spans or not w_spans:\n",
    "        return None, None, -1.0, \"missing_h_or_w\"\n",
    "\n",
    "    t = norm_text(text)\n",
    "    # índice de palabras clave\n",
    "    ctx_hits = [m.start() for m in context_words.finditer(t)]\n",
    "\n",
    "    best = (None, None, -1.0, \"\")\n",
    "    for h in h_spans:\n",
    "        for w in w_spans:\n",
    "            # distancia en caracteres\n",
    "            d = abs(h[4] - w[4])\n",
    "            # distancia a la palabra clave más cercana\n",
    "            if ctx_hits:\n",
    "                d_ctx = min(abs(h[4]-c) for c in ctx_hits) + min(abs(w[4]-c) for c in ctx_hits)\n",
    "            else:\n",
    "                d_ctx = 9999\n",
    "            # penaliza distancias grandes, favorece proximidad a contexto\n",
    "            score = 0.0\n",
    "            score += max(0, 200 - min(d, 200)) * 0.6\n",
    "            score += max(0, 200 - min(d_ctx, 200)) * 0.4\n",
    "            # bonifica formatos canónicos (m/cm y kg)\n",
    "            if \"cm\" in h[2].lower() or \" m\" in h[2].lower(): score += 10\n",
    "            if \"kg\" in w[2].lower(): score += 10\n",
    "\n",
    "            if score > best[2]:\n",
    "                best = (h, w, score, f\"d={d}, d_ctx={d_ctx}\")\n",
    "\n",
    "    return best\n",
    "\n",
    "# ================ Lectura del CSV y setup =================\n",
    "df = pd.read_csv(DATA_CSV, dtype=str, encoding=\"utf-8\", na_filter=False)\n",
    "if \"patient_id\" not in df.columns:\n",
    "    df[\"patient_id\"] = np.arange(len(df)).astype(str)\n",
    "df[\"patient_norm\"] = df[\"patient\"].apply(norm_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff592713",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando IMC (regex mejoradas): 100%|██████████| 167034/167034 [02:33<00:00, 1090.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Guardado:\n",
      " - C:\\Users\\hered\\Desktop\\TFM\\TFM\\IMC2\\valid_imc.csv (GT con IMC calculado)\n",
      " - C:\\Users\\hered\\Desktop\\TFM\\TFM\\IMC2\\doubtful_imc.csv (a revisar manualmente)\n",
      " - C:\\Users\\hered\\Desktop\\TFM\\TFM\\IMC2\\discarded_imc.csv (irrelevante)\n",
      "Ejemplo valid (head):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>height_m</th>\n",
       "      <th>height_raw</th>\n",
       "      <th>weight_kg</th>\n",
       "      <th>weight_raw</th>\n",
       "      <th>BMI</th>\n",
       "      <th>score</th>\n",
       "      <th>reason</th>\n",
       "      <th>snippet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>1.66</td>\n",
       "      <td>166.0 cm</td>\n",
       "      <td>48.4</td>\n",
       "      <td>48.4 kg</td>\n",
       "      <td>17.56</td>\n",
       "      <td>196.0</td>\n",
       "      <td>d=24, d_ctx=24</td>\n",
       "      <td>, 20; and temperature, 37.1℃. The weight of th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.75 m</td>\n",
       "      <td>76.0</td>\n",
       "      <td>76.0 kg</td>\n",
       "      <td>24.82</td>\n",
       "      <td>198.4</td>\n",
       "      <td>d=16, d_ctx=30</td>\n",
       "      <td>A 20-year-old Caucasian male (1.75 m tall and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>208</td>\n",
       "      <td>1.50</td>\n",
       "      <td>150.0 cm</td>\n",
       "      <td>41.0</td>\n",
       "      <td>41.0 kg</td>\n",
       "      <td>18.22</td>\n",
       "      <td>204.0</td>\n",
       "      <td>d=16, d_ctx=16</td>\n",
       "      <td>An 88-year-old woman (height, 150 cm; weight, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>428</td>\n",
       "      <td>1.60</td>\n",
       "      <td>160.0 cm</td>\n",
       "      <td>53.0</td>\n",
       "      <td>53.0 kg</td>\n",
       "      <td>20.70</td>\n",
       "      <td>125.6</td>\n",
       "      <td>d=24, d_ctx=848</td>\n",
       "      <td>rnal and Child Health Care Hospital. The pregn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>529</td>\n",
       "      <td>1.47</td>\n",
       "      <td>147.0 cm</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0 kg</td>\n",
       "      <td>23.14</td>\n",
       "      <td>204.0</td>\n",
       "      <td>d=16, d_ctx=16</td>\n",
       "      <td>A 36-year-old woman (height, 147 cm; weight, 5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  patient_id  height_m height_raw  weight_kg weight_raw    BMI  score  \\\n",
       "0         22      1.66   166.0 cm       48.4    48.4 kg  17.56  196.0   \n",
       "1         72      1.75     1.75 m       76.0    76.0 kg  24.82  198.4   \n",
       "2        208      1.50   150.0 cm       41.0    41.0 kg  18.22  204.0   \n",
       "3        428      1.60   160.0 cm       53.0    53.0 kg  20.70  125.6   \n",
       "4        529      1.47   147.0 cm       50.0    50.0 kg  23.14  204.0   \n",
       "\n",
       "            reason                                            snippet  \n",
       "0   d=24, d_ctx=24  , 20; and temperature, 37.1℃. The weight of th...  \n",
       "1   d=16, d_ctx=30  A 20-year-old Caucasian male (1.75 m tall and ...  \n",
       "2   d=16, d_ctx=16  An 88-year-old woman (height, 150 cm; weight, ...  \n",
       "3  d=24, d_ctx=848  rnal and Child Health Care Hospital. The pregn...  \n",
       "4   d=16, d_ctx=16  A 36-year-old woman (height, 147 cm; weight, 5...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid, doubtful, discarded = [], [], []\n",
    "\n",
    "for r in tqdm(df.itertuples(index=False), total=len(df), desc=\"Procesando IMC (regex mejoradas)\"):\n",
    "    pid   = getattr(r, \"patient_id\")\n",
    "    text  = getattr(r, \"patient_norm\")\n",
    "\n",
    "    h_spans = extract_heights(text)\n",
    "    w_spans = extract_weights(text)\n",
    "\n",
    "    if h_spans and w_spans:\n",
    "        h, w, score, info = pick_best_pair(h_spans, w_spans, text)\n",
    "        if (h is not None) and (w is not None):\n",
    "            h_m = h[1]; w_kg = w[1]\n",
    "            bmi = round(w_kg / (h_m*h_m), 2) if h_m and w_kg else None\n",
    "            if bmi is not None and math.isfinite(bmi):\n",
    "                # snippet combinando ambos\n",
    "                lo = max(0, min(h[4], w[4]) - 60)\n",
    "                hi = min(len(text), max(h[5], w[5]) + 60)\n",
    "                snippet = text[lo:hi]\n",
    "                valid.append({\n",
    "                    \"patient_id\": pid,\n",
    "                    \"height_m\": round(h_m, 3),\n",
    "                    \"height_raw\": h[2],\n",
    "                    \"weight_kg\": round(w_kg, 1),\n",
    "                    \"weight_raw\": w[2],\n",
    "                    \"BMI\": bmi,\n",
    "                    \"score\": round(score, 1),\n",
    "                    \"reason\": info,\n",
    "                    \"snippet\": snippet\n",
    "                })\n",
    "                continue  # siguiente paciente\n",
    "\n",
    "    # si llega aquí, no conseguimos pareja completa\n",
    "    if h_spans or w_spans:\n",
    "        doubtful.append({\n",
    "            \"patient_id\": pid,\n",
    "            \"heights\": json.dumps([(round(h[1],3), h[2]) for h in h_spans], ensure_ascii=False),\n",
    "            \"weights\": json.dumps([(round(w[1],1), w[2]) for w in w_spans], ensure_ascii=False),\n",
    "            \"text_excerpt\": text[:320]\n",
    "        })\n",
    "    else:\n",
    "        # había números pero no plausibles? los metemos en discarded\n",
    "        # (intentamos encontrar cualquier rastro para trazar)\n",
    "        any_nums = re.search(r\"\\b\\d[\\d.,]*\\b\", text) is not None\n",
    "        if any_nums:\n",
    "            discarded.append({\n",
    "                \"patient_id\": pid,\n",
    "                \"text_excerpt\": text[:320]\n",
    "            })\n",
    "\n",
    "# --- Guardar CSVs ---\n",
    "pd.DataFrame(valid).to_csv(OUT_DIR/\"valid_imc.csv\", index=False, encoding=\"utf-8\")\n",
    "pd.DataFrame(doubtful).to_csv(OUT_DIR/\"doubtful_imc.csv\", index=False, encoding=\"utf-8\")\n",
    "pd.DataFrame(discarded).to_csv(OUT_DIR/\"discarded_imc.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"✅ Guardado:\")\n",
    "print(f\" - {OUT_DIR/'valid_imc.csv'} (GT con IMC calculado)\")\n",
    "print(f\" - {OUT_DIR/'doubtful_imc.csv'} (a revisar manualmente)\")\n",
    "print(f\" - {OUT_DIR/'discarded_imc.csv'} (irrelevante)\")\n",
    "print(\"Ejemplo valid (head):\")\n",
    "pd.DataFrame(valid).head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10392ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved eval set: C:\\Users\\hered\\Desktop\\TFM\\TFM\\IMC2\\eval_imc_fullnotes.csv  | rows: 100\n",
      "patient_id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       full_note  height_m_true  weight_kg_true  BMI_true\n",
      "     72991                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     A 51-year-old (172 cm, 72.7 kg) man with a history of diffuse large B-cell lymphoma (DLBCL) presented with acute neurologic decline including a witnessed generalized tonic-clonic seizure prompting hospital admission. The patient underwent an extensive workup encompassing 28 days of persistent functional decline and progressive paralysis. The investigation concluded with a sural nerve biopsy that revealed neurolymphomatosis (DLBCL) with secondary nerve microvasculitis.\\nThe decision was made to treat with 8 grams/m2 and administer 15 grams of MTX based on a CG estimated creatinine clearance of >120 mL/min calculated using a serum creatinine of <0.4 mg/dL (comparable to available creatinine values in 1 month prior to admission) and body surface area (BSA) of 1.88 m2. Unfortunately, dysautonomia associated with the neurologic syndrome precipitated a hypotensive event requiring vasopressor support and a delay of chemotherapy. This acute decompensation prompted increased scrutiny of the patient's renal function beyond standard serum creatinine measurements. A cystatin C measurement returned at 1.81 mg/L, which indicated a CKD EPICr-CysC eGFR of 69 mL/min/1.73 m2 (70 mL/min). The MTX dose was subsequently decreased by 20% to 12 grams in accordance with the impaired renal function. MTX was administered followed by leucovorin rescue according to protocol []. Routine monitoring included daily serum creatinine and cystatin C concentrations, urine output, and serum MTX concentrations 48 hours after the infusion initiation and every 24 hours thereafter. On day +2 of MTX therapy, serum creatinine remained 0.4 mg/dL, cystatin C was 1.86 mg/L (CKD EPICr-CysC eGFR of 68 mL/min/1.73 m2, 74 mL/min), and the corresponding serum MTX level was 0.44 μmol/L (desired level < 0.1 μmol/L). Daily serum MTX concentrations are shown in . During this timeframe, the patient experienced mild hypokalemia, grade 1 elevation in aspartate aminotransferase without clinically evident symptoms. MTX concentrations were measured until a level of <0.1 μmol/L was achieved at day +6 at which point supportive care measures (leucovorin, intravenous fluids, sodium bicarbonate, and furosemide) were discontinued. Continued observation over the next 7 days demonstrated no change in the patient's neurologic status, and rituximab (375 mg/m2) in combination with 5 days of high-dose methylprednisolone (1 gram) were administered. Unfortunately, the patient still showed no improvement over the next several days. The decision was made to pursue comfort measures, and the patient expired soon after.           1.72            72.7     24.57\n",
      "    157400                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 A 73-year-old woman (height, 153 cm; weight, 53 kg), who underwent spine surgery at the same hospital as case 1 and 2, had mild subcutaneous emphysema on postoperative day 2. Pneumomediastinum and subcutaneous emphysema were presented on chest X-ray. Chest CT revealed a laceration on the posterior membranous wall of the mid-trachea (). However, the patient's symptoms were mild enough to recover by close observation with conservative treatment.           1.53            53.0     22.64\n",
      "    114398 In 2003, the Department of Oral Pathology and Surgery at the School of Dentistry, University of Athens referred a young 22-year-old female with Hajdu-Cheney Syndrome (HCS) to the Postgraduate Clinic of the Department of Periodontology, in order to receive periodontal treatment (Fig. ). The patient was diagnosed in 2001 with HCS after clinical, radiographic and histological examination (Fig. ). After gene examination of both parents, none of them was found bearing a mutation in the NOTCH2 gene []. Her physical examination showed that her height was 145cm and her weight was 45kg. The patient had thick coarse hair, low-set ears, small face and stature, thin lips, small mouth and short hands with clubbing of the fingertips (Fig. ). According to her medical history, the patient suffered from emphysema and allergic rhinitis, whereas oral intake of Vitamin D and calcium were prescribed daily for the treatment of osteoporosis [, ].\\nDuring orthodontic treatment between the ages of 12 and 21 years, all of her first premolars were removed []. At the age of 20 years, the patient received non-surgical periodontal treatment (scaling and root surface debridement), as well as limited periodontal surgery in the lower anterior region [, ]. Detailed records regarding both orthodontic and periodontal treatment were not available. The patient was a non-smoker, visited her dentist every 6 months, brushed her teeth twice every day (Bass technique) and used dental floss and interdental brushes. At the time of the referral, the patient suffered from generalized advanced chronic periodontitis, increased tooth mobility and premature tooth loss []. Clinical and radiographic examination of both parents and her 4 years younger brother showed that the mother and the younger brother had normal dentition, whereas the father was diagnosed with chronic advanced periodontal disease with increased mobility of various teeth.\\nThe patient was treated in collaboration with the Department of Prosthodontics. Clinical and radiographic examination of the patient revealed a number of significant findings. There was generalized horizontal bone loss of ~ 50%. However, alveolar bone loss around various teeth such as #4 (in place of #5), 14 and 24 was extensive (~ 100%). Tooth roots appeared short and cervical, whereas cervical resorption lesions were also evident (teeth #3, 10, 20 (in place of #19), 26) [, ]. The patient received non-surgical periodontal treatment in all four quadrants, which included scaling and root surface debridement. During this period, teeth #4, 14 and 24 were extracted due to extensive alveolar bone loss []. The maxilla was then rehabilitated with the placement of a provisional fixed partial denture of metal acrylic and the anterior region of the mandible with the placement of a Maryland bridge [] (Fig. ). Upon completion of the periodontal treatment, the patient was enrolled in a periodontal maintenance program.\\nAfter careful consideration of the possible implications deriving from the patient’s condition (osteoporosis, generalized advanced periodontitis) and having taken her young age into account, we decided to proceed with the placement of dental implants, while implementing specific protocols such as longer healing periods. So, although in the current literature no other case of implant placement in a patient with HCS was described, in February 2005, a dental implant (Nobel Biocare, Replace Select Straight, TiUnite RP 4.3 x 13mm) was placed in the upper right first premolar region [] (Figs. , ).\\nDuring the healing period, in March 2009, further teeth were extracted (#2,3,30). During the extraction of tooth #30, a bovine xenograft (Geistlich Bio-Oss Collagen 250 mg) was placed in the post-extraction site, in order to achieve ridge preservation [] and the region was rehabilitated with a resin-bonded bridge (Maryland bridge).\\nFive years after placement and successful osseointegration of the dental implant in position #5, an additional implant (Nobel Biocare, Replace Select Tapered TiUnite Regular Platform (RP) 4.3 x 8mm) was placed in the upper right first molar region (Fig. ). Bone mineral density appeared physiological (Bone Type III) (Fig. ).\\nSix months after implant placement, a porcelain fused to metal, three unit implant supported fixed partial denture 3 (4) 5 was placed in the upper right region (Fig. ). During this period of time, teeth # 18, 19 were extracted due to excessive bone loss.\\nClinical and radiographic examination of the patient during the periodontal maintenance program in three months interval after implant placement revealed no abnormalities in the implant region. After successful oral rehabilitation of the posterior upper right region, 2 additional dental implants were placed in the posterior left region (during surgery tooth #20 - in place of #21- was extracted) and 1 in the posterior right region of the mandible (Straumann Standard Platform (SP) Tissue Level) (Figs. -).\\nSix months after implant placement, a porcelain fused to metal, three unit implant supported fixed partial denture [] 19 18 was placed in the left region of the mandible, whereas a porcelain fused to metal implant supported crown was placed in the right region of the mandible (Fig. ).\\nFive years after implant placement, clinical and radiographic examination of the patient during the periodontal maintenance program (in three months interval) revealed no abnormalities (Figs. -).\\nFor the reader’s better understanding, Table presents the patient’s detailed dental treatment chronologically.           1.45            45.0     21.40\n"
     ]
    }
   ],
   "source": [
    "# === Celda 1: construir eval_imc_fullnotes.csv (100 muestras) ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Rutas\n",
    "PMC_CSV   = r\"C:/Users/hered/Desktop/TFM/TFM/data/PMC-Patients.csv\"\n",
    "VALID_CSV = r\"C:/Users/hered/Desktop/TFM/TFM/IMC2/valid_imc.csv\"\n",
    "OUT_DIR   = Path(r\"C:/Users/hered/Desktop/TFM/TFM/IMC2\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Cargar\n",
    "df_pmc  = pd.read_csv(PMC_CSV,  dtype=str, encoding=\"utf-8\", na_filter=False)\n",
    "df_val  = pd.read_csv(VALID_CSV, dtype=str, encoding=\"utf-8\", na_filter=False)\n",
    "\n",
    "# patient_id asegurado\n",
    "if \"patient_id\" not in df_pmc.columns:\n",
    "    df_pmc[\"patient_id\"] = np.arange(len(df_pmc)).astype(str)\n",
    "\n",
    "# Tipar numéricos GT\n",
    "for c in [\"height_m\",\"weight_kg\",\"BMI\"]:\n",
    "    if c in df_val.columns:\n",
    "        df_val[c] = pd.to_numeric(df_val[c], errors=\"coerce\")\n",
    "\n",
    "# Muestra aleatoria de 100\n",
    "rng = np.random.default_rng(42)\n",
    "idx = rng.choice(df_val.index.to_numpy(), size=min(100, len(df_val)), replace=False)\n",
    "sample = df_val.loc[idx].copy()\n",
    "\n",
    "# Unir con fullnotes\n",
    "df_pmc[\"full_note\"] = df_pmc[\"patient\"]  # renombrado conveniente\n",
    "keep_cols = [\"patient_id\",\"full_note\"]\n",
    "eval_df = sample.merge(df_pmc[keep_cols], on=\"patient_id\", how=\"left\")\n",
    "\n",
    "# Renombrar a *_true para MEDCALC\n",
    "eval_df = eval_df.rename(columns={\n",
    "    \"height_m\":\"height_m_true\",\n",
    "    \"weight_kg\":\"weight_kg_true\",\n",
    "    \"BMI\":\"BMI_true\"\n",
    "})\n",
    "\n",
    "# Orden amigable\n",
    "cols = [\"patient_id\",\"full_note\",\"height_m_true\",\"weight_kg_true\",\"BMI_true\"]\n",
    "for c in cols:\n",
    "    if c not in eval_df.columns:\n",
    "        eval_df[c] = np.nan\n",
    "eval_df = eval_df[cols]\n",
    "\n",
    "# Guardar\n",
    "OUT_EVAL = OUT_DIR / \"eval_imc_fullnotes.csv\"\n",
    "eval_df.to_csv(OUT_EVAL, index=False, encoding=\"utf-8\")\n",
    "print(f\"✅ Saved eval set: {OUT_EVAL}  | rows: {len(eval_df)}\")\n",
    "print(eval_df.head(3).to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
